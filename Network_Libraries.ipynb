{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLX21yR3Gez2T8yAKN1SEd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyBrain Network\n",
        "\n"
      ],
      "metadata": {
        "id": "XzgLV-7fB-9f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzqFaguCs8T-"
      },
      "outputs": [],
      "source": [
        "!pip install https://github.com/pybrain/pybrain/archive/0.3.3.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pybrain"
      ],
      "metadata": {
        "id": "N7OfT-7SChNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pybrain.structure import FeedForwardNetwork\n",
        "from pybrain.structure import SigmoidLayer, LinearLayer, BiasUnit\n",
        "from pybrain.structure import FullConnection\n",
        "\n",
        "#feed forward is standard input to hidden to output\n",
        "#fullconnection is so that all neurons in one layer is connected to the neurons in the next layer"
      ],
      "metadata": {
        "id": "W9vaZYkpDTRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = FeedForwardNetwork()"
      ],
      "metadata": {
        "id": "fIRnUE_2D2iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for 2 inputs, so no activation function\n",
        "input_layer = LinearLayer(2)\n",
        "\n",
        "\n",
        "#sigmoid 3 neurons hidden layer\n",
        "\n",
        "hidden_layer = SigmoidLayer(3)\n",
        "\n",
        "#output 1 neuron\n",
        "\n",
        "output_layer = SigmoidLayer(1)\n",
        "\n",
        "#bias unit for hidden layer and then output layer\n",
        "bias0 = BiasUnit()\n",
        "bias1 = BiasUnit()"
      ],
      "metadata": {
        "id": "i5QtQfY7D8dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add above variables with above network\n",
        "\n",
        "network.addModule(input_layer)\n",
        "network.addModule(hidden_layer)\n",
        "network.addModule(output_layer)\n",
        "network.addModule(bias0)\n",
        "network.addModule(bias1)"
      ],
      "metadata": {
        "id": "qzAoB5SHEk_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#connect input to hidden layer\n",
        "input_to_hidden = FullConnection(input_layer, hidden_layer)\n",
        "\n",
        "#connect hidden to output\n",
        "hidden_to_output = FullConnection(hidden_layer, output_layer)\n",
        "\n",
        "#add baises\n",
        "\n",
        "bias_hidden = FullConnection(bias0, hidden_layer)\n",
        "\n",
        "bias_output = FullConnection(bias1, output_layer)"
      ],
      "metadata": {
        "id": "erv37MmpE2zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build whole network\n",
        "network.sortModules()"
      ],
      "metadata": {
        "id": "2INV1uIeFRcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize\n",
        "print(network)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IL18GjlFXwW",
        "outputId": "e453207f-4482-4c93-d134-e0bc497f91a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForwardNetwork-10\n",
            "   Modules:\n",
            "    [<BiasUnit 'BiasUnit-6'>, <BiasUnit 'BiasUnit-9'>, <LinearLayer 'LinearLayer-5'>, <SigmoidLayer 'SigmoidLayer-7'>, <SigmoidLayer 'SigmoidLayer-8'>]\n",
            "   Connections:\n",
            "    []\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#displays input to hidden layer weights - first 3 are from 1st input, 2nd 3 are from 2nd input\n",
        "print(input_to_hidden.params)\n",
        "#displays hidden to output layer weights\n",
        "print(hidden_to_output.params)\n",
        "#displays biases\n",
        "print(bias_hidden.params)\n",
        "print(bias_output.params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nCqjMYiFjHd",
        "outputId": "10d8ab28-0a54-4780-f461-9ff2097a5cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.09837991  0.01079438 -1.69359331  0.85254611 -1.36372221  0.26268664]\n",
            "[ 1.4807916   1.43792094 -0.11145034]\n",
            "[0.40856584 0.17001246 0.75844098]\n",
            "[0.0259064]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can speed up the process of making a network, lets do so now"
      ],
      "metadata": {
        "id": "bH0VTWKdGivd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pybrain.tools.shortcuts import buildNetwork\n",
        "from pybrain.datasets import SupervisedDataSet\n",
        "from pybrain.supervised import BackpropTrainer\n",
        "from pybrain.structure.modules import SigmoidLayer"
      ],
      "metadata": {
        "id": "USETRWyqGm_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import FALSE\n",
        "#basic network, with 2 input, 3 hidden layer, 1 output neuron layer with sigmoid function as linearlayer is default\n",
        "network_1 = buildNetwork(2, 3, 1, outclass = SigmoidLayer, hiddenclass = SigmoidLayer, bias = False)\n",
        "network_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQrELoiYHNGT",
        "outputId": "3c69d44d-8dbc-49c9-ce12-c7096b1981ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<FeedForwardNetwork 'FeedForwardNetwork-63'>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(network_1[\"in\"])\n",
        "print(network_1[\"hidden0\"])\n",
        "print(network_1[\"out\"]) # <- want sigmoid output\n",
        "print(network_1[\"bias\"]) # <- no bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW8e4UzcHl0b",
        "outputId": "e8645ee6-2e2b-4efa-c850-49089dd29ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<LinearLayer 'in'>\n",
            "<SigmoidLayer 'hidden0'>\n",
            "<SigmoidLayer 'out'>\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = SupervisedDataSet(2, 1)  #<- 2 inputs 1 output these makes a sample dataset\n",
        "dataset.addSample((0,0), (0,))\n",
        "dataset.addSample((1,0), (1,))\n",
        "dataset.addSample((0,1), (1,))\n",
        "dataset.addSample((1,1), (0,))"
      ],
      "metadata": {
        "id": "z4Fh7n1sIuu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['input'])\n",
        "print(dataset['target'])\n",
        "optimizer = BackpropTrainer(module = network_1, dataset = dataset, learningrate = 0.1)\n",
        "epochs = 50000\n",
        "error = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  error_average = optimizer.train()\n",
        "  if epoch % 10000 == 0:\n",
        "    print(\"Epoch: \" + str(epoch+1) + \"Error: \" + str(error_average))\n",
        "    error.append(error_average)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-IeFvKEI213",
        "outputId": "58269528-b796-49b0-8fd2-8f51efa55c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [1. 1.]]\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "Epoch: 1Error: 0.05880383034018402\n",
            "Epoch: 10001Error: 0.04029936184233575\n",
            "Epoch: 20001Error: 0.009548356279687625\n",
            "Epoch: 30001Error: 0.0050042642899171055\n",
            "Epoch: 40001Error: 0.0033181022738949626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"error\")\n",
        "plt.plot(error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "7eaCxVqTKCTj",
        "outputId": "60fad3d7-8130-47b5-eca7-3067d495f29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9ddb4257b0>]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network_1.params\n",
        "#shows network weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6cr94fCKTvX",
        "outputId": "5a280bf6-0dfb-433f-9a78-b3cb6837ca6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  5.87691721,   6.66202248, -10.55729504,   5.1713495 ,\n",
              "        -0.58534843,   0.89226924,  11.8464583 ,   8.90900795,\n",
              "       -25.41616627])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #test inputs\n",
        "print(network_1.activate([0,0]))\n",
        "print(network_1.activate([0,1]))\n",
        "#works good nice"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR_WH7iPKaIq",
        "outputId": "caf3d343-e1bf-46dc-815a-1054e1d1fb6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08864038]\n",
            "[0.93467854]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SK Learn Network"
      ],
      "metadata": {
        "id": "YWsyBuYlLH2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supports other machine learning models, not just neural networks."
      ],
      "metadata": {
        "id": "z8trRGa-BoDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier #multi-layer perceptron\n",
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "OqNbDcDrL3Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()"
      ],
      "metadata": {
        "id": "Qvww5gurB8qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = iris.data\n",
        "iris.feature_names\n",
        "\n",
        "outputs = iris.target #will give the classifications\n",
        "print(iris.target_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InkyE0JXB_-g",
        "outputId": "6802ca76-4916-46ee-f543-849053d04a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['setosa' 'versicolor' 'virginica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs.shape)\n",
        "print(outputs.shape) #not necessary to change vector to matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycGL_vmOCihq",
        "outputId": "a3dc3b60-9b32-4498-fdc2-1e10bba75f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 4)\n",
            "(150,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cross validation we must do on our own\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(inputs, outputs, test_size = 0.2) #80% train and 20% test\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape) #120 for train, 30 for testing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6dDEvkGCu1S",
        "outputId": "5030077f-5db9-4ce7-ea7c-c6a1fb5bfc87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(120, 4)\n",
            "(120,)\n",
            "(30, 4)\n",
            "(30,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import VERBOSE\n",
        "#network training\n",
        "network = MLPClassifier(max_iter = 3000, verbose = True, tol=0.0000100, activation = \"logistic\", solver = \"adam\", learning_rate = \"constant\",  learning_rate_init=0.001\n",
        "                        , batch_size=32, hidden_layer_sizes= (4,4))#, early_stopping = True, n_iter_no_change= 50) #can set your own activation functions shown in sci kit learn documentation\n",
        "network.fit(x_train, y_train)  #adam was the optimizer, learning rate can also have different options\n",
        "#outputs and inputs don't need to be declared, but whats in hidden layers ought to be declared\n",
        "#layer size, (4,4) means 4 inputs to 4 hidden layer neurons to again 4 neuron layer again to 3 outputs\n",
        "#can also set the abort sequence so that if there isn't improvement after x number of epochs, it'll abort and stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IXsbTvAVDxa9",
        "outputId": "b780c96d-54f5-4f1e-ffae-61053e4165aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.14630902\n",
            "Iteration 2, loss = 1.14189256\n",
            "Iteration 3, loss = 1.13753629\n",
            "Iteration 4, loss = 1.13337368\n",
            "Iteration 5, loss = 1.12952569\n",
            "Iteration 6, loss = 1.12613659\n",
            "Iteration 7, loss = 1.12276309\n",
            "Iteration 8, loss = 1.11944679\n",
            "Iteration 9, loss = 1.11721199\n",
            "Iteration 10, loss = 1.11380522\n",
            "Iteration 11, loss = 1.11143899\n",
            "Iteration 12, loss = 1.10899802\n",
            "Iteration 13, loss = 1.10697854\n",
            "Iteration 14, loss = 1.10469547\n",
            "Iteration 15, loss = 1.10265722\n",
            "Iteration 16, loss = 1.10146748\n",
            "Iteration 17, loss = 1.09945511\n",
            "Iteration 18, loss = 1.09753231\n",
            "Iteration 19, loss = 1.09578254\n",
            "Iteration 20, loss = 1.09468135\n",
            "Iteration 21, loss = 1.09310413\n",
            "Iteration 22, loss = 1.09162943\n",
            "Iteration 23, loss = 1.09045835\n",
            "Iteration 24, loss = 1.08919787\n",
            "Iteration 25, loss = 1.08776916\n",
            "Iteration 26, loss = 1.08681597\n",
            "Iteration 27, loss = 1.08567703\n",
            "Iteration 28, loss = 1.08423274\n",
            "Iteration 29, loss = 1.08309017\n",
            "Iteration 30, loss = 1.08197500\n",
            "Iteration 31, loss = 1.08091661\n",
            "Iteration 32, loss = 1.07982212\n",
            "Iteration 33, loss = 1.07839168\n",
            "Iteration 34, loss = 1.07724336\n",
            "Iteration 35, loss = 1.07605562\n",
            "Iteration 36, loss = 1.07497160\n",
            "Iteration 37, loss = 1.07384525\n",
            "Iteration 38, loss = 1.07245132\n",
            "Iteration 39, loss = 1.07117879\n",
            "Iteration 40, loss = 1.06985551\n",
            "Iteration 41, loss = 1.06868059\n",
            "Iteration 42, loss = 1.06718992\n",
            "Iteration 43, loss = 1.06580621\n",
            "Iteration 44, loss = 1.06449446\n",
            "Iteration 45, loss = 1.06296531\n",
            "Iteration 46, loss = 1.06149953\n",
            "Iteration 47, loss = 1.05990524\n",
            "Iteration 48, loss = 1.05827831\n",
            "Iteration 49, loss = 1.05664458\n",
            "Iteration 50, loss = 1.05498819\n",
            "Iteration 51, loss = 1.05320977\n",
            "Iteration 52, loss = 1.05135106\n",
            "Iteration 53, loss = 1.04958576\n",
            "Iteration 54, loss = 1.04757509\n",
            "Iteration 55, loss = 1.04559354\n",
            "Iteration 56, loss = 1.04361597\n",
            "Iteration 57, loss = 1.04142633\n",
            "Iteration 58, loss = 1.03932795\n",
            "Iteration 59, loss = 1.03718638\n",
            "Iteration 60, loss = 1.03496025\n",
            "Iteration 61, loss = 1.03279844\n",
            "Iteration 62, loss = 1.03046040\n",
            "Iteration 63, loss = 1.02810302\n",
            "Iteration 64, loss = 1.02579237\n",
            "Iteration 65, loss = 1.02341052\n",
            "Iteration 66, loss = 1.02098286\n",
            "Iteration 67, loss = 1.01857741\n",
            "Iteration 68, loss = 1.01613722\n",
            "Iteration 69, loss = 1.01361932\n",
            "Iteration 70, loss = 1.01102904\n",
            "Iteration 71, loss = 1.00851027\n",
            "Iteration 72, loss = 1.00589319\n",
            "Iteration 73, loss = 1.00318064\n",
            "Iteration 74, loss = 1.00061881\n",
            "Iteration 75, loss = 0.99780827\n",
            "Iteration 76, loss = 0.99510668\n",
            "Iteration 77, loss = 0.99255059\n",
            "Iteration 78, loss = 0.98956862\n",
            "Iteration 79, loss = 0.98664692\n",
            "Iteration 80, loss = 0.98374544\n",
            "Iteration 81, loss = 0.98085826\n",
            "Iteration 82, loss = 0.97803960\n",
            "Iteration 83, loss = 0.97501845\n",
            "Iteration 84, loss = 0.97194832\n",
            "Iteration 85, loss = 0.96894885\n",
            "Iteration 86, loss = 0.96582746\n",
            "Iteration 87, loss = 0.96276434\n",
            "Iteration 88, loss = 0.95960406\n",
            "Iteration 89, loss = 0.95660287\n",
            "Iteration 90, loss = 0.95323828\n",
            "Iteration 91, loss = 0.95013244\n",
            "Iteration 92, loss = 0.94689095\n",
            "Iteration 93, loss = 0.94362512\n",
            "Iteration 94, loss = 0.94029349\n",
            "Iteration 95, loss = 0.93708527\n",
            "Iteration 96, loss = 0.93366956\n",
            "Iteration 97, loss = 0.93031440\n",
            "Iteration 98, loss = 0.92698938\n",
            "Iteration 99, loss = 0.92357473\n",
            "Iteration 100, loss = 0.92021807\n",
            "Iteration 101, loss = 0.91673325\n",
            "Iteration 102, loss = 0.91332166\n",
            "Iteration 103, loss = 0.90988135\n",
            "Iteration 104, loss = 0.90641165\n",
            "Iteration 105, loss = 0.90297118\n",
            "Iteration 106, loss = 0.89950652\n",
            "Iteration 107, loss = 0.89593112\n",
            "Iteration 108, loss = 0.89247157\n",
            "Iteration 109, loss = 0.88895905\n",
            "Iteration 110, loss = 0.88542749\n",
            "Iteration 111, loss = 0.88187360\n",
            "Iteration 112, loss = 0.87837466\n",
            "Iteration 113, loss = 0.87488229\n",
            "Iteration 114, loss = 0.87131066\n",
            "Iteration 115, loss = 0.86789960\n",
            "Iteration 116, loss = 0.86419664\n",
            "Iteration 117, loss = 0.86081238\n",
            "Iteration 118, loss = 0.85718214\n",
            "Iteration 119, loss = 0.85372937\n",
            "Iteration 120, loss = 0.85010732\n",
            "Iteration 121, loss = 0.84659766\n",
            "Iteration 122, loss = 0.84317260\n",
            "Iteration 123, loss = 0.83964036\n",
            "Iteration 124, loss = 0.83615994\n",
            "Iteration 125, loss = 0.83277443\n",
            "Iteration 126, loss = 0.82930248\n",
            "Iteration 127, loss = 0.82573011\n",
            "Iteration 128, loss = 0.82230315\n",
            "Iteration 129, loss = 0.81910197\n",
            "Iteration 130, loss = 0.81547696\n",
            "Iteration 131, loss = 0.81211534\n",
            "Iteration 132, loss = 0.80876833\n",
            "Iteration 133, loss = 0.80543566\n",
            "Iteration 134, loss = 0.80208237\n",
            "Iteration 135, loss = 0.79880448\n",
            "Iteration 136, loss = 0.79546396\n",
            "Iteration 137, loss = 0.79223193\n",
            "Iteration 138, loss = 0.78895430\n",
            "Iteration 139, loss = 0.78569581\n",
            "Iteration 140, loss = 0.78247155\n",
            "Iteration 141, loss = 0.77929734\n",
            "Iteration 142, loss = 0.77616080\n",
            "Iteration 143, loss = 0.77304616\n",
            "Iteration 144, loss = 0.76981277\n",
            "Iteration 145, loss = 0.76674919\n",
            "Iteration 146, loss = 0.76367397\n",
            "Iteration 147, loss = 0.76082836\n",
            "Iteration 148, loss = 0.75753735\n",
            "Iteration 149, loss = 0.75461813\n",
            "Iteration 150, loss = 0.75167300\n",
            "Iteration 151, loss = 0.74866067\n",
            "Iteration 152, loss = 0.74577239\n",
            "Iteration 153, loss = 0.74288936\n",
            "Iteration 154, loss = 0.74005969\n",
            "Iteration 155, loss = 0.73715717\n",
            "Iteration 156, loss = 0.73435198\n",
            "Iteration 157, loss = 0.73151354\n",
            "Iteration 158, loss = 0.72886856\n",
            "Iteration 159, loss = 0.72604231\n",
            "Iteration 160, loss = 0.72341413\n",
            "Iteration 161, loss = 0.72069494\n",
            "Iteration 162, loss = 0.71807218\n",
            "Iteration 163, loss = 0.71546783\n",
            "Iteration 164, loss = 0.71285937\n",
            "Iteration 165, loss = 0.71031875\n",
            "Iteration 166, loss = 0.70777589\n",
            "Iteration 167, loss = 0.70533618\n",
            "Iteration 168, loss = 0.70276428\n",
            "Iteration 169, loss = 0.70030919\n",
            "Iteration 170, loss = 0.69795420\n",
            "Iteration 171, loss = 0.69545923\n",
            "Iteration 172, loss = 0.69314120\n",
            "Iteration 173, loss = 0.69076925\n",
            "Iteration 174, loss = 0.68859020\n",
            "Iteration 175, loss = 0.68623740\n",
            "Iteration 176, loss = 0.68388692\n",
            "Iteration 177, loss = 0.68163936\n",
            "Iteration 178, loss = 0.67943795\n",
            "Iteration 179, loss = 0.67734848\n",
            "Iteration 180, loss = 0.67503903\n",
            "Iteration 181, loss = 0.67288652\n",
            "Iteration 182, loss = 0.67079478\n",
            "Iteration 183, loss = 0.66870947\n",
            "Iteration 184, loss = 0.66659901\n",
            "Iteration 185, loss = 0.66454277\n",
            "Iteration 186, loss = 0.66258190\n",
            "Iteration 187, loss = 0.66054486\n",
            "Iteration 188, loss = 0.65849660\n",
            "Iteration 189, loss = 0.65660744\n",
            "Iteration 190, loss = 0.65469569\n",
            "Iteration 191, loss = 0.65289821\n",
            "Iteration 192, loss = 0.65079429\n",
            "Iteration 193, loss = 0.64892764\n",
            "Iteration 194, loss = 0.64711616\n",
            "Iteration 195, loss = 0.64527145\n",
            "Iteration 196, loss = 0.64347343\n",
            "Iteration 197, loss = 0.64170868\n",
            "Iteration 198, loss = 0.63990928\n",
            "Iteration 199, loss = 0.63817612\n",
            "Iteration 200, loss = 0.63648912\n",
            "Iteration 201, loss = 0.63474957\n",
            "Iteration 202, loss = 0.63317312\n",
            "Iteration 203, loss = 0.63153986\n",
            "Iteration 204, loss = 0.62978369\n",
            "Iteration 205, loss = 0.62826123\n",
            "Iteration 206, loss = 0.62657752\n",
            "Iteration 207, loss = 0.62498647\n",
            "Iteration 208, loss = 0.62343407\n",
            "Iteration 209, loss = 0.62191945\n",
            "Iteration 210, loss = 0.62034794\n",
            "Iteration 211, loss = 0.61886831\n",
            "Iteration 212, loss = 0.61739887\n",
            "Iteration 213, loss = 0.61595896\n",
            "Iteration 214, loss = 0.61441472\n",
            "Iteration 215, loss = 0.61298123\n",
            "Iteration 216, loss = 0.61152091\n",
            "Iteration 217, loss = 0.61015256\n",
            "Iteration 218, loss = 0.60874989\n",
            "Iteration 219, loss = 0.60738460\n",
            "Iteration 220, loss = 0.60603287\n",
            "Iteration 221, loss = 0.60472462\n",
            "Iteration 222, loss = 0.60346099\n",
            "Iteration 223, loss = 0.60205014\n",
            "Iteration 224, loss = 0.60073597\n",
            "Iteration 225, loss = 0.59953850\n",
            "Iteration 226, loss = 0.59820970\n",
            "Iteration 227, loss = 0.59692830\n",
            "Iteration 228, loss = 0.59567972\n",
            "Iteration 229, loss = 0.59444628\n",
            "Iteration 230, loss = 0.59324432\n",
            "Iteration 231, loss = 0.59202512\n",
            "Iteration 232, loss = 0.59083932\n",
            "Iteration 233, loss = 0.58963760\n",
            "Iteration 234, loss = 0.58850371\n",
            "Iteration 235, loss = 0.58735916\n",
            "Iteration 236, loss = 0.58616269\n",
            "Iteration 237, loss = 0.58505071\n",
            "Iteration 238, loss = 0.58397505\n",
            "Iteration 239, loss = 0.58307299\n",
            "Iteration 240, loss = 0.58181565\n",
            "Iteration 241, loss = 0.58063414\n",
            "Iteration 242, loss = 0.57955306\n",
            "Iteration 243, loss = 0.57850538\n",
            "Iteration 244, loss = 0.57751057\n",
            "Iteration 245, loss = 0.57645839\n",
            "Iteration 246, loss = 0.57538635\n",
            "Iteration 247, loss = 0.57435925\n",
            "Iteration 248, loss = 0.57331587\n",
            "Iteration 249, loss = 0.57235200\n",
            "Iteration 250, loss = 0.57133807\n",
            "Iteration 251, loss = 0.57034955\n",
            "Iteration 252, loss = 0.56937058\n",
            "Iteration 253, loss = 0.56837786\n",
            "Iteration 254, loss = 0.56744392\n",
            "Iteration 255, loss = 0.56649859\n",
            "Iteration 256, loss = 0.56552333\n",
            "Iteration 257, loss = 0.56458857\n",
            "Iteration 258, loss = 0.56368487\n",
            "Iteration 259, loss = 0.56276581\n",
            "Iteration 260, loss = 0.56180140\n",
            "Iteration 261, loss = 0.56092712\n",
            "Iteration 262, loss = 0.55998121\n",
            "Iteration 263, loss = 0.55915706\n",
            "Iteration 264, loss = 0.55829575\n",
            "Iteration 265, loss = 0.55732973\n",
            "Iteration 266, loss = 0.55645369\n",
            "Iteration 267, loss = 0.55560626\n",
            "Iteration 268, loss = 0.55475096\n",
            "Iteration 269, loss = 0.55408883\n",
            "Iteration 270, loss = 0.55308198\n",
            "Iteration 271, loss = 0.55223120\n",
            "Iteration 272, loss = 0.55143912\n",
            "Iteration 273, loss = 0.55048942\n",
            "Iteration 274, loss = 0.54975507\n",
            "Iteration 275, loss = 0.54885923\n",
            "Iteration 276, loss = 0.54802185\n",
            "Iteration 277, loss = 0.54716826\n",
            "Iteration 278, loss = 0.54635747\n",
            "Iteration 279, loss = 0.54556474\n",
            "Iteration 280, loss = 0.54471170\n",
            "Iteration 281, loss = 0.54393125\n",
            "Iteration 282, loss = 0.54310730\n",
            "Iteration 283, loss = 0.54230077\n",
            "Iteration 284, loss = 0.54150937\n",
            "Iteration 285, loss = 0.54079022\n",
            "Iteration 286, loss = 0.53993215\n",
            "Iteration 287, loss = 0.53913265\n",
            "Iteration 288, loss = 0.53834558\n",
            "Iteration 289, loss = 0.53758158\n",
            "Iteration 290, loss = 0.53679986\n",
            "Iteration 291, loss = 0.53599162\n",
            "Iteration 292, loss = 0.53526997\n",
            "Iteration 293, loss = 0.53444213\n",
            "Iteration 294, loss = 0.53368267\n",
            "Iteration 295, loss = 0.53292367\n",
            "Iteration 296, loss = 0.53212257\n",
            "Iteration 297, loss = 0.53135487\n",
            "Iteration 298, loss = 0.53065273\n",
            "Iteration 299, loss = 0.52984409\n",
            "Iteration 300, loss = 0.52911287\n",
            "Iteration 301, loss = 0.52838136\n",
            "Iteration 302, loss = 0.52753863\n",
            "Iteration 303, loss = 0.52676420\n",
            "Iteration 304, loss = 0.52601944\n",
            "Iteration 305, loss = 0.52521640\n",
            "Iteration 306, loss = 0.52444887\n",
            "Iteration 307, loss = 0.52367143\n",
            "Iteration 308, loss = 0.52290867\n",
            "Iteration 309, loss = 0.52218296\n",
            "Iteration 310, loss = 0.52143432\n",
            "Iteration 311, loss = 0.52060089\n",
            "Iteration 312, loss = 0.51987780\n",
            "Iteration 313, loss = 0.51916055\n",
            "Iteration 314, loss = 0.51829318\n",
            "Iteration 315, loss = 0.51759814\n",
            "Iteration 316, loss = 0.51672267\n",
            "Iteration 317, loss = 0.51592836\n",
            "Iteration 318, loss = 0.51516079\n",
            "Iteration 319, loss = 0.51437072\n",
            "Iteration 320, loss = 0.51357723\n",
            "Iteration 321, loss = 0.51279280\n",
            "Iteration 322, loss = 0.51202116\n",
            "Iteration 323, loss = 0.51123060\n",
            "Iteration 324, loss = 0.51042890\n",
            "Iteration 325, loss = 0.50963427\n",
            "Iteration 326, loss = 0.50888494\n",
            "Iteration 327, loss = 0.50800802\n",
            "Iteration 328, loss = 0.50721137\n",
            "Iteration 329, loss = 0.50650546\n",
            "Iteration 330, loss = 0.50566708\n",
            "Iteration 331, loss = 0.50475870\n",
            "Iteration 332, loss = 0.50394488\n",
            "Iteration 333, loss = 0.50312142\n",
            "Iteration 334, loss = 0.50229222\n",
            "Iteration 335, loss = 0.50146796\n",
            "Iteration 336, loss = 0.50064649\n",
            "Iteration 337, loss = 0.49981766\n",
            "Iteration 338, loss = 0.49896767\n",
            "Iteration 339, loss = 0.49812224\n",
            "Iteration 340, loss = 0.49731491\n",
            "Iteration 341, loss = 0.49647254\n",
            "Iteration 342, loss = 0.49559890\n",
            "Iteration 343, loss = 0.49476112\n",
            "Iteration 344, loss = 0.49384375\n",
            "Iteration 345, loss = 0.49300650\n",
            "Iteration 346, loss = 0.49214007\n",
            "Iteration 347, loss = 0.49132744\n",
            "Iteration 348, loss = 0.49043733\n",
            "Iteration 349, loss = 0.48956756\n",
            "Iteration 350, loss = 0.48873908\n",
            "Iteration 351, loss = 0.48772854\n",
            "Iteration 352, loss = 0.48696648\n",
            "Iteration 353, loss = 0.48594543\n",
            "Iteration 354, loss = 0.48501748\n",
            "Iteration 355, loss = 0.48414387\n",
            "Iteration 356, loss = 0.48320238\n",
            "Iteration 357, loss = 0.48231292\n",
            "Iteration 358, loss = 0.48135280\n",
            "Iteration 359, loss = 0.48049186\n",
            "Iteration 360, loss = 0.47956105\n",
            "Iteration 361, loss = 0.47861838\n",
            "Iteration 362, loss = 0.47768241\n",
            "Iteration 363, loss = 0.47674533\n",
            "Iteration 364, loss = 0.47580989\n",
            "Iteration 365, loss = 0.47489845\n",
            "Iteration 366, loss = 0.47390262\n",
            "Iteration 367, loss = 0.47302205\n",
            "Iteration 368, loss = 0.47200083\n",
            "Iteration 369, loss = 0.47114962\n",
            "Iteration 370, loss = 0.47008094\n",
            "Iteration 371, loss = 0.46916946\n",
            "Iteration 372, loss = 0.46818734\n",
            "Iteration 373, loss = 0.46713820\n",
            "Iteration 374, loss = 0.46619132\n",
            "Iteration 375, loss = 0.46539650\n",
            "Iteration 376, loss = 0.46425981\n",
            "Iteration 377, loss = 0.46343993\n",
            "Iteration 378, loss = 0.46231664\n",
            "Iteration 379, loss = 0.46125683\n",
            "Iteration 380, loss = 0.46024337\n",
            "Iteration 381, loss = 0.45924230\n",
            "Iteration 382, loss = 0.45842636\n",
            "Iteration 383, loss = 0.45730796\n",
            "Iteration 384, loss = 0.45629048\n",
            "Iteration 385, loss = 0.45523912\n",
            "Iteration 386, loss = 0.45426028\n",
            "Iteration 387, loss = 0.45339985\n",
            "Iteration 388, loss = 0.45220877\n",
            "Iteration 389, loss = 0.45121335\n",
            "Iteration 390, loss = 0.45019326\n",
            "Iteration 391, loss = 0.44917874\n",
            "Iteration 392, loss = 0.44817309\n",
            "Iteration 393, loss = 0.44726310\n",
            "Iteration 394, loss = 0.44601595\n",
            "Iteration 395, loss = 0.44499589\n",
            "Iteration 396, loss = 0.44391611\n",
            "Iteration 397, loss = 0.44291270\n",
            "Iteration 398, loss = 0.44191922\n",
            "Iteration 399, loss = 0.44080784\n",
            "Iteration 400, loss = 0.43971712\n",
            "Iteration 401, loss = 0.43875110\n",
            "Iteration 402, loss = 0.43763354\n",
            "Iteration 403, loss = 0.43663148\n",
            "Iteration 404, loss = 0.43548902\n",
            "Iteration 405, loss = 0.43439400\n",
            "Iteration 406, loss = 0.43337233\n",
            "Iteration 407, loss = 0.43235396\n",
            "Iteration 408, loss = 0.43119918\n",
            "Iteration 409, loss = 0.43019788\n",
            "Iteration 410, loss = 0.42917874\n",
            "Iteration 411, loss = 0.42798397\n",
            "Iteration 412, loss = 0.42684984\n",
            "Iteration 413, loss = 0.42580057\n",
            "Iteration 414, loss = 0.42486116\n",
            "Iteration 415, loss = 0.42359313\n",
            "Iteration 416, loss = 0.42266135\n",
            "Iteration 417, loss = 0.42141590\n",
            "Iteration 418, loss = 0.42042833\n",
            "Iteration 419, loss = 0.41928390\n",
            "Iteration 420, loss = 0.41823654\n",
            "Iteration 421, loss = 0.41715276\n",
            "Iteration 422, loss = 0.41599097\n",
            "Iteration 423, loss = 0.41503815\n",
            "Iteration 424, loss = 0.41384063\n",
            "Iteration 425, loss = 0.41284972\n",
            "Iteration 426, loss = 0.41176500\n",
            "Iteration 427, loss = 0.41067157\n",
            "Iteration 428, loss = 0.40943220\n",
            "Iteration 429, loss = 0.40831513\n",
            "Iteration 430, loss = 0.40735811\n",
            "Iteration 431, loss = 0.40610020\n",
            "Iteration 432, loss = 0.40503015\n",
            "Iteration 433, loss = 0.40395999\n",
            "Iteration 434, loss = 0.40290059\n",
            "Iteration 435, loss = 0.40187887\n",
            "Iteration 436, loss = 0.40068957\n",
            "Iteration 437, loss = 0.39957120\n",
            "Iteration 438, loss = 0.39843244\n",
            "Iteration 439, loss = 0.39749221\n",
            "Iteration 440, loss = 0.39627259\n",
            "Iteration 441, loss = 0.39518556\n",
            "Iteration 442, loss = 0.39417680\n",
            "Iteration 443, loss = 0.39301395\n",
            "Iteration 444, loss = 0.39193083\n",
            "Iteration 445, loss = 0.39083482\n",
            "Iteration 446, loss = 0.38972660\n",
            "Iteration 447, loss = 0.38858782\n",
            "Iteration 448, loss = 0.38751515\n",
            "Iteration 449, loss = 0.38641787\n",
            "Iteration 450, loss = 0.38544456\n",
            "Iteration 451, loss = 0.38424849\n",
            "Iteration 452, loss = 0.38309872\n",
            "Iteration 453, loss = 0.38205621\n",
            "Iteration 454, loss = 0.38105930\n",
            "Iteration 455, loss = 0.37982056\n",
            "Iteration 456, loss = 0.37879431\n",
            "Iteration 457, loss = 0.37764443\n",
            "Iteration 458, loss = 0.37664007\n",
            "Iteration 459, loss = 0.37557109\n",
            "Iteration 460, loss = 0.37440174\n",
            "Iteration 461, loss = 0.37326133\n",
            "Iteration 462, loss = 0.37214775\n",
            "Iteration 463, loss = 0.37106104\n",
            "Iteration 464, loss = 0.37004727\n",
            "Iteration 465, loss = 0.36904514\n",
            "Iteration 466, loss = 0.36783487\n",
            "Iteration 467, loss = 0.36703994\n",
            "Iteration 468, loss = 0.36568682\n",
            "Iteration 469, loss = 0.36458224\n",
            "Iteration 470, loss = 0.36366154\n",
            "Iteration 471, loss = 0.36247401\n",
            "Iteration 472, loss = 0.36135679\n",
            "Iteration 473, loss = 0.36035127\n",
            "Iteration 474, loss = 0.35927230\n",
            "Iteration 475, loss = 0.35832234\n",
            "Iteration 476, loss = 0.35715659\n",
            "Iteration 477, loss = 0.35602655\n",
            "Iteration 478, loss = 0.35497390\n",
            "Iteration 479, loss = 0.35387875\n",
            "Iteration 480, loss = 0.35291812\n",
            "Iteration 481, loss = 0.35231070\n",
            "Iteration 482, loss = 0.35075757\n",
            "Iteration 483, loss = 0.34970336\n",
            "Iteration 484, loss = 0.34873724\n",
            "Iteration 485, loss = 0.34764991\n",
            "Iteration 486, loss = 0.34655327\n",
            "Iteration 487, loss = 0.34557517\n",
            "Iteration 488, loss = 0.34450876\n",
            "Iteration 489, loss = 0.34343121\n",
            "Iteration 490, loss = 0.34246170\n",
            "Iteration 491, loss = 0.34158313\n",
            "Iteration 492, loss = 0.34071792\n",
            "Iteration 493, loss = 0.33942238\n",
            "Iteration 494, loss = 0.33831492\n",
            "Iteration 495, loss = 0.33740044\n",
            "Iteration 496, loss = 0.33625936\n",
            "Iteration 497, loss = 0.33528405\n",
            "Iteration 498, loss = 0.33432722\n",
            "Iteration 499, loss = 0.33343251\n",
            "Iteration 500, loss = 0.33229271\n",
            "Iteration 501, loss = 0.33126538\n",
            "Iteration 502, loss = 0.33026926\n",
            "Iteration 503, loss = 0.32938234\n",
            "Iteration 504, loss = 0.32833776\n",
            "Iteration 505, loss = 0.32723959\n",
            "Iteration 506, loss = 0.32638468\n",
            "Iteration 507, loss = 0.32532675\n",
            "Iteration 508, loss = 0.32438828\n",
            "Iteration 509, loss = 0.32331303\n",
            "Iteration 510, loss = 0.32240341\n",
            "Iteration 511, loss = 0.32152584\n",
            "Iteration 512, loss = 0.32044586\n",
            "Iteration 513, loss = 0.31944652\n",
            "Iteration 514, loss = 0.31842177\n",
            "Iteration 515, loss = 0.31748800\n",
            "Iteration 516, loss = 0.31656347\n",
            "Iteration 517, loss = 0.31566158\n",
            "Iteration 518, loss = 0.31466827\n",
            "Iteration 519, loss = 0.31368146\n",
            "Iteration 520, loss = 0.31283915\n",
            "Iteration 521, loss = 0.31190979\n",
            "Iteration 522, loss = 0.31099364\n",
            "Iteration 523, loss = 0.30996176\n",
            "Iteration 524, loss = 0.30900182\n",
            "Iteration 525, loss = 0.30810797\n",
            "Iteration 526, loss = 0.30714483\n",
            "Iteration 527, loss = 0.30630594\n",
            "Iteration 528, loss = 0.30542891\n",
            "Iteration 529, loss = 0.30429135\n",
            "Iteration 530, loss = 0.30342216\n",
            "Iteration 531, loss = 0.30243320\n",
            "Iteration 532, loss = 0.30162023\n",
            "Iteration 533, loss = 0.30119155\n",
            "Iteration 534, loss = 0.29991991\n",
            "Iteration 535, loss = 0.29890668\n",
            "Iteration 536, loss = 0.29791658\n",
            "Iteration 537, loss = 0.29704622\n",
            "Iteration 538, loss = 0.29616706\n",
            "Iteration 539, loss = 0.29534793\n",
            "Iteration 540, loss = 0.29443874\n",
            "Iteration 541, loss = 0.29352461\n",
            "Iteration 542, loss = 0.29273441\n",
            "Iteration 543, loss = 0.29173585\n",
            "Iteration 544, loss = 0.29095835\n",
            "Iteration 545, loss = 0.28998567\n",
            "Iteration 546, loss = 0.28916221\n",
            "Iteration 547, loss = 0.28833553\n",
            "Iteration 548, loss = 0.28740158\n",
            "Iteration 549, loss = 0.28669295\n",
            "Iteration 550, loss = 0.28569584\n",
            "Iteration 551, loss = 0.28481156\n",
            "Iteration 552, loss = 0.28409838\n",
            "Iteration 553, loss = 0.28312893\n",
            "Iteration 554, loss = 0.28218760\n",
            "Iteration 555, loss = 0.28155496\n",
            "Iteration 556, loss = 0.28075022\n",
            "Iteration 557, loss = 0.27975156\n",
            "Iteration 558, loss = 0.27892843\n",
            "Iteration 559, loss = 0.27812104\n",
            "Iteration 560, loss = 0.27725880\n",
            "Iteration 561, loss = 0.27650083\n",
            "Iteration 562, loss = 0.27570614\n",
            "Iteration 563, loss = 0.27478274\n",
            "Iteration 564, loss = 0.27396474\n",
            "Iteration 565, loss = 0.27319075\n",
            "Iteration 566, loss = 0.27231813\n",
            "Iteration 567, loss = 0.27152012\n",
            "Iteration 568, loss = 0.27073094\n",
            "Iteration 569, loss = 0.26994947\n",
            "Iteration 570, loss = 0.26926348\n",
            "Iteration 571, loss = 0.26847204\n",
            "Iteration 572, loss = 0.26757598\n",
            "Iteration 573, loss = 0.26673215\n",
            "Iteration 574, loss = 0.26592268\n",
            "Iteration 575, loss = 0.26513748\n",
            "Iteration 576, loss = 0.26440494\n",
            "Iteration 577, loss = 0.26382427\n",
            "Iteration 578, loss = 0.26292884\n",
            "Iteration 579, loss = 0.26207532\n",
            "Iteration 580, loss = 0.26129115\n",
            "Iteration 581, loss = 0.26050587\n",
            "Iteration 582, loss = 0.25973545\n",
            "Iteration 583, loss = 0.25893941\n",
            "Iteration 584, loss = 0.25816852\n",
            "Iteration 585, loss = 0.25743458\n",
            "Iteration 586, loss = 0.25677484\n",
            "Iteration 587, loss = 0.25607404\n",
            "Iteration 588, loss = 0.25519022\n",
            "Iteration 589, loss = 0.25451211\n",
            "Iteration 590, loss = 0.25381944\n",
            "Iteration 591, loss = 0.25314962\n",
            "Iteration 592, loss = 0.25222322\n",
            "Iteration 593, loss = 0.25168716\n",
            "Iteration 594, loss = 0.25084479\n",
            "Iteration 595, loss = 0.25022329\n",
            "Iteration 596, loss = 0.24924602\n",
            "Iteration 597, loss = 0.24866045\n",
            "Iteration 598, loss = 0.24786189\n",
            "Iteration 599, loss = 0.24720448\n",
            "Iteration 600, loss = 0.24642840\n",
            "Iteration 601, loss = 0.24577277\n",
            "Iteration 602, loss = 0.24501148\n",
            "Iteration 603, loss = 0.24437777\n",
            "Iteration 604, loss = 0.24352164\n",
            "Iteration 605, loss = 0.24286833\n",
            "Iteration 606, loss = 0.24211843\n",
            "Iteration 607, loss = 0.24143897\n",
            "Iteration 608, loss = 0.24077804\n",
            "Iteration 609, loss = 0.24007805\n",
            "Iteration 610, loss = 0.23943834\n",
            "Iteration 611, loss = 0.23868792\n",
            "Iteration 612, loss = 0.23812528\n",
            "Iteration 613, loss = 0.23746970\n",
            "Iteration 614, loss = 0.23672522\n",
            "Iteration 615, loss = 0.23604105\n",
            "Iteration 616, loss = 0.23530141\n",
            "Iteration 617, loss = 0.23465884\n",
            "Iteration 618, loss = 0.23407576\n",
            "Iteration 619, loss = 0.23360437\n",
            "Iteration 620, loss = 0.23258769\n",
            "Iteration 621, loss = 0.23193243\n",
            "Iteration 622, loss = 0.23153028\n",
            "Iteration 623, loss = 0.23079412\n",
            "Iteration 624, loss = 0.23013685\n",
            "Iteration 625, loss = 0.22952216\n",
            "Iteration 626, loss = 0.22875741\n",
            "Iteration 627, loss = 0.22817099\n",
            "Iteration 628, loss = 0.22744981\n",
            "Iteration 629, loss = 0.22730671\n",
            "Iteration 630, loss = 0.22627914\n",
            "Iteration 631, loss = 0.22563140\n",
            "Iteration 632, loss = 0.22501469\n",
            "Iteration 633, loss = 0.22426442\n",
            "Iteration 634, loss = 0.22372492\n",
            "Iteration 635, loss = 0.22307824\n",
            "Iteration 636, loss = 0.22246864\n",
            "Iteration 637, loss = 0.22183381\n",
            "Iteration 638, loss = 0.22131684\n",
            "Iteration 639, loss = 0.22057921\n",
            "Iteration 640, loss = 0.21997530\n",
            "Iteration 641, loss = 0.21946299\n",
            "Iteration 642, loss = 0.21874219\n",
            "Iteration 643, loss = 0.21816163\n",
            "Iteration 644, loss = 0.21750524\n",
            "Iteration 645, loss = 0.21692660\n",
            "Iteration 646, loss = 0.21650934\n",
            "Iteration 647, loss = 0.21572541\n",
            "Iteration 648, loss = 0.21515187\n",
            "Iteration 649, loss = 0.21453853\n",
            "Iteration 650, loss = 0.21392724\n",
            "Iteration 651, loss = 0.21349576\n",
            "Iteration 652, loss = 0.21279333\n",
            "Iteration 653, loss = 0.21221760\n",
            "Iteration 654, loss = 0.21163655\n",
            "Iteration 655, loss = 0.21108502\n",
            "Iteration 656, loss = 0.21079807\n",
            "Iteration 657, loss = 0.20995516\n",
            "Iteration 658, loss = 0.20945095\n",
            "Iteration 659, loss = 0.20879573\n",
            "Iteration 660, loss = 0.20833369\n",
            "Iteration 661, loss = 0.20808323\n",
            "Iteration 662, loss = 0.20705597\n",
            "Iteration 663, loss = 0.20660096\n",
            "Iteration 664, loss = 0.20599674\n",
            "Iteration 665, loss = 0.20540923\n",
            "Iteration 666, loss = 0.20491002\n",
            "Iteration 667, loss = 0.20437951\n",
            "Iteration 668, loss = 0.20376680\n",
            "Iteration 669, loss = 0.20323204\n",
            "Iteration 670, loss = 0.20268634\n",
            "Iteration 671, loss = 0.20214912\n",
            "Iteration 672, loss = 0.20158733\n",
            "Iteration 673, loss = 0.20119110\n",
            "Iteration 674, loss = 0.20054924\n",
            "Iteration 675, loss = 0.20000600\n",
            "Iteration 676, loss = 0.19947491\n",
            "Iteration 677, loss = 0.19899937\n",
            "Iteration 678, loss = 0.19851480\n",
            "Iteration 679, loss = 0.19794303\n",
            "Iteration 680, loss = 0.19733981\n",
            "Iteration 681, loss = 0.19679636\n",
            "Iteration 682, loss = 0.19631807\n",
            "Iteration 683, loss = 0.19589582\n",
            "Iteration 684, loss = 0.19534345\n",
            "Iteration 685, loss = 0.19485880\n",
            "Iteration 686, loss = 0.19429760\n",
            "Iteration 687, loss = 0.19375267\n",
            "Iteration 688, loss = 0.19340026\n",
            "Iteration 689, loss = 0.19279705\n",
            "Iteration 690, loss = 0.19230417\n",
            "Iteration 691, loss = 0.19197932\n",
            "Iteration 692, loss = 0.19125669\n",
            "Iteration 693, loss = 0.19081811\n",
            "Iteration 694, loss = 0.19034421\n",
            "Iteration 695, loss = 0.18988076\n",
            "Iteration 696, loss = 0.18942567\n",
            "Iteration 697, loss = 0.18888020\n",
            "Iteration 698, loss = 0.18841545\n",
            "Iteration 699, loss = 0.18808706\n",
            "Iteration 700, loss = 0.18736740\n",
            "Iteration 701, loss = 0.18693926\n",
            "Iteration 702, loss = 0.18656049\n",
            "Iteration 703, loss = 0.18600011\n",
            "Iteration 704, loss = 0.18547547\n",
            "Iteration 705, loss = 0.18503339\n",
            "Iteration 706, loss = 0.18459688\n",
            "Iteration 707, loss = 0.18427758\n",
            "Iteration 708, loss = 0.18373664\n",
            "Iteration 709, loss = 0.18310676\n",
            "Iteration 710, loss = 0.18278767\n",
            "Iteration 711, loss = 0.18220366\n",
            "Iteration 712, loss = 0.18182143\n",
            "Iteration 713, loss = 0.18126532\n",
            "Iteration 714, loss = 0.18082862\n",
            "Iteration 715, loss = 0.18039128\n",
            "Iteration 716, loss = 0.17999283\n",
            "Iteration 717, loss = 0.17968715\n",
            "Iteration 718, loss = 0.17909484\n",
            "Iteration 719, loss = 0.17867173\n",
            "Iteration 720, loss = 0.17832562\n",
            "Iteration 721, loss = 0.17773881\n",
            "Iteration 722, loss = 0.17730972\n",
            "Iteration 723, loss = 0.17701647\n",
            "Iteration 724, loss = 0.17646119\n",
            "Iteration 725, loss = 0.17595637\n",
            "Iteration 726, loss = 0.17554823\n",
            "Iteration 727, loss = 0.17515496\n",
            "Iteration 728, loss = 0.17487109\n",
            "Iteration 729, loss = 0.17446766\n",
            "Iteration 730, loss = 0.17394379\n",
            "Iteration 731, loss = 0.17339035\n",
            "Iteration 732, loss = 0.17305086\n",
            "Iteration 733, loss = 0.17266980\n",
            "Iteration 734, loss = 0.17212899\n",
            "Iteration 735, loss = 0.17179342\n",
            "Iteration 736, loss = 0.17129992\n",
            "Iteration 737, loss = 0.17102895\n",
            "Iteration 738, loss = 0.17060427\n",
            "Iteration 739, loss = 0.17003476\n",
            "Iteration 740, loss = 0.16972099\n",
            "Iteration 741, loss = 0.16930948\n",
            "Iteration 742, loss = 0.16907893\n",
            "Iteration 743, loss = 0.16895572\n",
            "Iteration 744, loss = 0.16803785\n",
            "Iteration 745, loss = 0.16764371\n",
            "Iteration 746, loss = 0.16724011\n",
            "Iteration 747, loss = 0.16682016\n",
            "Iteration 748, loss = 0.16649055\n",
            "Iteration 749, loss = 0.16602781\n",
            "Iteration 750, loss = 0.16563142\n",
            "Iteration 751, loss = 0.16549600\n",
            "Iteration 752, loss = 0.16485800\n",
            "Iteration 753, loss = 0.16452051\n",
            "Iteration 754, loss = 0.16434194\n",
            "Iteration 755, loss = 0.16363772\n",
            "Iteration 756, loss = 0.16332422\n",
            "Iteration 757, loss = 0.16300760\n",
            "Iteration 758, loss = 0.16266630\n",
            "Iteration 759, loss = 0.16219445\n",
            "Iteration 760, loss = 0.16208291\n",
            "Iteration 761, loss = 0.16157172\n",
            "Iteration 762, loss = 0.16101958\n",
            "Iteration 763, loss = 0.16076355\n",
            "Iteration 764, loss = 0.16043628\n",
            "Iteration 765, loss = 0.16014083\n",
            "Iteration 766, loss = 0.15969189\n",
            "Iteration 767, loss = 0.15918424\n",
            "Iteration 768, loss = 0.15885495\n",
            "Iteration 769, loss = 0.15842969\n",
            "Iteration 770, loss = 0.15811138\n",
            "Iteration 771, loss = 0.15811317\n",
            "Iteration 772, loss = 0.15753223\n",
            "Iteration 773, loss = 0.15700735\n",
            "Iteration 774, loss = 0.15686219\n",
            "Iteration 775, loss = 0.15628535\n",
            "Iteration 776, loss = 0.15590964\n",
            "Iteration 777, loss = 0.15558515\n",
            "Iteration 778, loss = 0.15549878\n",
            "Iteration 779, loss = 0.15493482\n",
            "Iteration 780, loss = 0.15450886\n",
            "Iteration 781, loss = 0.15413793\n",
            "Iteration 782, loss = 0.15399939\n",
            "Iteration 783, loss = 0.15343091\n",
            "Iteration 784, loss = 0.15341712\n",
            "Iteration 785, loss = 0.15292168\n",
            "Iteration 786, loss = 0.15249290\n",
            "Iteration 787, loss = 0.15205878\n",
            "Iteration 788, loss = 0.15183596\n",
            "Iteration 789, loss = 0.15145140\n",
            "Iteration 790, loss = 0.15115444\n",
            "Iteration 791, loss = 0.15073611\n",
            "Iteration 792, loss = 0.15044602\n",
            "Iteration 793, loss = 0.15019273\n",
            "Iteration 794, loss = 0.14986461\n",
            "Iteration 795, loss = 0.14943881\n",
            "Iteration 796, loss = 0.14909428\n",
            "Iteration 797, loss = 0.14873482\n",
            "Iteration 798, loss = 0.14841548\n",
            "Iteration 799, loss = 0.14839815\n",
            "Iteration 800, loss = 0.14780997\n",
            "Iteration 801, loss = 0.14749351\n",
            "Iteration 802, loss = 0.14711485\n",
            "Iteration 803, loss = 0.14680150\n",
            "Iteration 804, loss = 0.14645872\n",
            "Iteration 805, loss = 0.14619526\n",
            "Iteration 806, loss = 0.14586706\n",
            "Iteration 807, loss = 0.14550914\n",
            "Iteration 808, loss = 0.14523824\n",
            "Iteration 809, loss = 0.14493471\n",
            "Iteration 810, loss = 0.14466022\n",
            "Iteration 811, loss = 0.14433641\n",
            "Iteration 812, loss = 0.14397973\n",
            "Iteration 813, loss = 0.14365272\n",
            "Iteration 814, loss = 0.14339251\n",
            "Iteration 815, loss = 0.14302544\n",
            "Iteration 816, loss = 0.14273349\n",
            "Iteration 817, loss = 0.14259236\n",
            "Iteration 818, loss = 0.14227844\n",
            "Iteration 819, loss = 0.14179402\n",
            "Iteration 820, loss = 0.14158326\n",
            "Iteration 821, loss = 0.14118418\n",
            "Iteration 822, loss = 0.14094506\n",
            "Iteration 823, loss = 0.14069386\n",
            "Iteration 824, loss = 0.14028242\n",
            "Iteration 825, loss = 0.14006344\n",
            "Iteration 826, loss = 0.13971745\n",
            "Iteration 827, loss = 0.13948727\n",
            "Iteration 828, loss = 0.13915761\n",
            "Iteration 829, loss = 0.13884463\n",
            "Iteration 830, loss = 0.13858450\n",
            "Iteration 831, loss = 0.13871903\n",
            "Iteration 832, loss = 0.13799591\n",
            "Iteration 833, loss = 0.13766053\n",
            "Iteration 834, loss = 0.13743940\n",
            "Iteration 835, loss = 0.13705780\n",
            "Iteration 836, loss = 0.13695277\n",
            "Iteration 837, loss = 0.13677464\n",
            "Iteration 838, loss = 0.13621179\n",
            "Iteration 839, loss = 0.13601279\n",
            "Iteration 840, loss = 0.13573962\n",
            "Iteration 841, loss = 0.13565479\n",
            "Iteration 842, loss = 0.13518447\n",
            "Iteration 843, loss = 0.13491368\n",
            "Iteration 844, loss = 0.13456921\n",
            "Iteration 845, loss = 0.13431371\n",
            "Iteration 846, loss = 0.13416591\n",
            "Iteration 847, loss = 0.13381563\n",
            "Iteration 848, loss = 0.13351916\n",
            "Iteration 849, loss = 0.13327715\n",
            "Iteration 850, loss = 0.13300398\n",
            "Iteration 851, loss = 0.13276368\n",
            "Iteration 852, loss = 0.13244914\n",
            "Iteration 853, loss = 0.13220095\n",
            "Iteration 854, loss = 0.13191657\n",
            "Iteration 855, loss = 0.13161615\n",
            "Iteration 856, loss = 0.13163346\n",
            "Iteration 857, loss = 0.13136114\n",
            "Iteration 858, loss = 0.13086151\n",
            "Iteration 859, loss = 0.13060877\n",
            "Iteration 860, loss = 0.13036049\n",
            "Iteration 861, loss = 0.13012131\n",
            "Iteration 862, loss = 0.12984790\n",
            "Iteration 863, loss = 0.12954823\n",
            "Iteration 864, loss = 0.12919750\n",
            "Iteration 865, loss = 0.12915663\n",
            "Iteration 866, loss = 0.12885605\n",
            "Iteration 867, loss = 0.12874310\n",
            "Iteration 868, loss = 0.12835954\n",
            "Iteration 869, loss = 0.12819285\n",
            "Iteration 870, loss = 0.12783482\n",
            "Iteration 871, loss = 0.12750790\n",
            "Iteration 872, loss = 0.12727677\n",
            "Iteration 873, loss = 0.12700259\n",
            "Iteration 874, loss = 0.12682081\n",
            "Iteration 875, loss = 0.12657938\n",
            "Iteration 876, loss = 0.12632776\n",
            "Iteration 877, loss = 0.12614893\n",
            "Iteration 878, loss = 0.12575604\n",
            "Iteration 879, loss = 0.12550877\n",
            "Iteration 880, loss = 0.12530936\n",
            "Iteration 881, loss = 0.12528478\n",
            "Iteration 882, loss = 0.12491162\n",
            "Iteration 883, loss = 0.12463566\n",
            "Iteration 884, loss = 0.12431911\n",
            "Iteration 885, loss = 0.12419792\n",
            "Iteration 886, loss = 0.12407461\n",
            "Iteration 887, loss = 0.12383922\n",
            "Iteration 888, loss = 0.12387713\n",
            "Iteration 889, loss = 0.12315264\n",
            "Iteration 890, loss = 0.12319970\n",
            "Iteration 891, loss = 0.12303767\n",
            "Iteration 892, loss = 0.12263061\n",
            "Iteration 893, loss = 0.12236312\n",
            "Iteration 894, loss = 0.12202055\n",
            "Iteration 895, loss = 0.12181862\n",
            "Iteration 896, loss = 0.12160604\n",
            "Iteration 897, loss = 0.12146502\n",
            "Iteration 898, loss = 0.12110828\n",
            "Iteration 899, loss = 0.12097655\n",
            "Iteration 900, loss = 0.12066832\n",
            "Iteration 901, loss = 0.12043932\n",
            "Iteration 902, loss = 0.12037666\n",
            "Iteration 903, loss = 0.12043502\n",
            "Iteration 904, loss = 0.11991668\n",
            "Iteration 905, loss = 0.11969795\n",
            "Iteration 906, loss = 0.11951340\n",
            "Iteration 907, loss = 0.12000168\n",
            "Iteration 908, loss = 0.11895484\n",
            "Iteration 909, loss = 0.11871681\n",
            "Iteration 910, loss = 0.11845906\n",
            "Iteration 911, loss = 0.11824489\n",
            "Iteration 912, loss = 0.11808766\n",
            "Iteration 913, loss = 0.11780207\n",
            "Iteration 914, loss = 0.11758890\n",
            "Iteration 915, loss = 0.11748058\n",
            "Iteration 916, loss = 0.11756045\n",
            "Iteration 917, loss = 0.11696589\n",
            "Iteration 918, loss = 0.11678766\n",
            "Iteration 919, loss = 0.11656032\n",
            "Iteration 920, loss = 0.11641720\n",
            "Iteration 921, loss = 0.11619745\n",
            "Iteration 922, loss = 0.11617878\n",
            "Iteration 923, loss = 0.11583406\n",
            "Iteration 924, loss = 0.11548752\n",
            "Iteration 925, loss = 0.11528521\n",
            "Iteration 926, loss = 0.11520497\n",
            "Iteration 927, loss = 0.11507309\n",
            "Iteration 928, loss = 0.11474720\n",
            "Iteration 929, loss = 0.11449821\n",
            "Iteration 930, loss = 0.11426266\n",
            "Iteration 931, loss = 0.11472953\n",
            "Iteration 932, loss = 0.11402829\n",
            "Iteration 933, loss = 0.11369085\n",
            "Iteration 934, loss = 0.11350594\n",
            "Iteration 935, loss = 0.11353177\n",
            "Iteration 936, loss = 0.11349032\n",
            "Iteration 937, loss = 0.11288561\n",
            "Iteration 938, loss = 0.11278690\n",
            "Iteration 939, loss = 0.11264538\n",
            "Iteration 940, loss = 0.11235607\n",
            "Iteration 941, loss = 0.11210534\n",
            "Iteration 942, loss = 0.11206359\n",
            "Iteration 943, loss = 0.11175160\n",
            "Iteration 944, loss = 0.11153529\n",
            "Iteration 945, loss = 0.11138373\n",
            "Iteration 946, loss = 0.11122626\n",
            "Iteration 947, loss = 0.11113295\n",
            "Iteration 948, loss = 0.11097114\n",
            "Iteration 949, loss = 0.11061508\n",
            "Iteration 950, loss = 0.11036661\n",
            "Iteration 951, loss = 0.11022263\n",
            "Iteration 952, loss = 0.11017397\n",
            "Iteration 953, loss = 0.10981837\n",
            "Iteration 954, loss = 0.10972029\n",
            "Iteration 955, loss = 0.10958732\n",
            "Iteration 956, loss = 0.10933187\n",
            "Iteration 957, loss = 0.10919186\n",
            "Iteration 958, loss = 0.10887152\n",
            "Iteration 959, loss = 0.10879495\n",
            "Iteration 960, loss = 0.10883748\n",
            "Iteration 961, loss = 0.10850322\n",
            "Iteration 962, loss = 0.10826616\n",
            "Iteration 963, loss = 0.10797200\n",
            "Iteration 964, loss = 0.10778985\n",
            "Iteration 965, loss = 0.10762116\n",
            "Iteration 966, loss = 0.10750580\n",
            "Iteration 967, loss = 0.10745662\n",
            "Iteration 968, loss = 0.10737424\n",
            "Iteration 969, loss = 0.10692990\n",
            "Iteration 970, loss = 0.10696070\n",
            "Iteration 971, loss = 0.10656354\n",
            "Iteration 972, loss = 0.10641251\n",
            "Iteration 973, loss = 0.10622586\n",
            "Iteration 974, loss = 0.10604154\n",
            "Iteration 975, loss = 0.10601117\n",
            "Iteration 976, loss = 0.10569520\n",
            "Iteration 977, loss = 0.10564813\n",
            "Iteration 978, loss = 0.10551232\n",
            "Iteration 979, loss = 0.10519795\n",
            "Iteration 980, loss = 0.10509450\n",
            "Iteration 981, loss = 0.10488293\n",
            "Iteration 982, loss = 0.10480582\n",
            "Iteration 983, loss = 0.10452947\n",
            "Iteration 984, loss = 0.10430097\n",
            "Iteration 985, loss = 0.10429102\n",
            "Iteration 986, loss = 0.10404345\n",
            "Iteration 987, loss = 0.10391254\n",
            "Iteration 988, loss = 0.10367827\n",
            "Iteration 989, loss = 0.10359070\n",
            "Iteration 990, loss = 0.10341576\n",
            "Iteration 991, loss = 0.10318454\n",
            "Iteration 992, loss = 0.10311129\n",
            "Iteration 993, loss = 0.10299907\n",
            "Iteration 994, loss = 0.10268944\n",
            "Iteration 995, loss = 0.10245369\n",
            "Iteration 996, loss = 0.10229085\n",
            "Iteration 997, loss = 0.10236401\n",
            "Iteration 998, loss = 0.10200083\n",
            "Iteration 999, loss = 0.10191621\n",
            "Iteration 1000, loss = 0.10180626\n",
            "Iteration 1001, loss = 0.10158168\n",
            "Iteration 1002, loss = 0.10151196\n",
            "Iteration 1003, loss = 0.10134430\n",
            "Iteration 1004, loss = 0.10116360\n",
            "Iteration 1005, loss = 0.10104897\n",
            "Iteration 1006, loss = 0.10085470\n",
            "Iteration 1007, loss = 0.10078386\n",
            "Iteration 1008, loss = 0.10056031\n",
            "Iteration 1009, loss = 0.10025981\n",
            "Iteration 1010, loss = 0.10010395\n",
            "Iteration 1011, loss = 0.10016643\n",
            "Iteration 1012, loss = 0.09982680\n",
            "Iteration 1013, loss = 0.09987353\n",
            "Iteration 1014, loss = 0.09969084\n",
            "Iteration 1015, loss = 0.09937797\n",
            "Iteration 1016, loss = 0.09912450\n",
            "Iteration 1017, loss = 0.09909753\n",
            "Iteration 1018, loss = 0.09902136\n",
            "Iteration 1019, loss = 0.09881488\n",
            "Iteration 1020, loss = 0.09878569\n",
            "Iteration 1021, loss = 0.09850394\n",
            "Iteration 1022, loss = 0.09826170\n",
            "Iteration 1023, loss = 0.09821737\n",
            "Iteration 1024, loss = 0.09812738\n",
            "Iteration 1025, loss = 0.09797891\n",
            "Iteration 1026, loss = 0.09780763\n",
            "Iteration 1027, loss = 0.09754765\n",
            "Iteration 1028, loss = 0.09759879\n",
            "Iteration 1029, loss = 0.09732373\n",
            "Iteration 1030, loss = 0.09708480\n",
            "Iteration 1031, loss = 0.09716198\n",
            "Iteration 1032, loss = 0.09683978\n",
            "Iteration 1033, loss = 0.09666372\n",
            "Iteration 1034, loss = 0.09659582\n",
            "Iteration 1035, loss = 0.09642352\n",
            "Iteration 1036, loss = 0.09628151\n",
            "Iteration 1037, loss = 0.09611296\n",
            "Iteration 1038, loss = 0.09602663\n",
            "Iteration 1039, loss = 0.09588547\n",
            "Iteration 1040, loss = 0.09574474\n",
            "Iteration 1041, loss = 0.09553361\n",
            "Iteration 1042, loss = 0.09542004\n",
            "Iteration 1043, loss = 0.09527241\n",
            "Iteration 1044, loss = 0.09515289\n",
            "Iteration 1045, loss = 0.09498666\n",
            "Iteration 1046, loss = 0.09519619\n",
            "Iteration 1047, loss = 0.09473876\n",
            "Iteration 1048, loss = 0.09467718\n",
            "Iteration 1049, loss = 0.09446558\n",
            "Iteration 1050, loss = 0.09447396\n",
            "Iteration 1051, loss = 0.09418543\n",
            "Iteration 1052, loss = 0.09404298\n",
            "Iteration 1053, loss = 0.09394225\n",
            "Iteration 1054, loss = 0.09394158\n",
            "Iteration 1055, loss = 0.09358845\n",
            "Iteration 1056, loss = 0.09352382\n",
            "Iteration 1057, loss = 0.09353867\n",
            "Iteration 1058, loss = 0.09326418\n",
            "Iteration 1059, loss = 0.09310146\n",
            "Iteration 1060, loss = 0.09291676\n",
            "Iteration 1061, loss = 0.09281544\n",
            "Iteration 1062, loss = 0.09267410\n",
            "Iteration 1063, loss = 0.09282937\n",
            "Iteration 1064, loss = 0.09255706\n",
            "Iteration 1065, loss = 0.09234008\n",
            "Iteration 1066, loss = 0.09214236\n",
            "Iteration 1067, loss = 0.09204247\n",
            "Iteration 1068, loss = 0.09204122\n",
            "Iteration 1069, loss = 0.09180209\n",
            "Iteration 1070, loss = 0.09183585\n",
            "Iteration 1071, loss = 0.09152192\n",
            "Iteration 1072, loss = 0.09146803\n",
            "Iteration 1073, loss = 0.09125237\n",
            "Iteration 1074, loss = 0.09109819\n",
            "Iteration 1075, loss = 0.09096365\n",
            "Iteration 1076, loss = 0.09091881\n",
            "Iteration 1077, loss = 0.09075388\n",
            "Iteration 1078, loss = 0.09070900\n",
            "Iteration 1079, loss = 0.09052419\n",
            "Iteration 1080, loss = 0.09041315\n",
            "Iteration 1081, loss = 0.09023512\n",
            "Iteration 1082, loss = 0.09022772\n",
            "Iteration 1083, loss = 0.09001000\n",
            "Iteration 1084, loss = 0.08983224\n",
            "Iteration 1085, loss = 0.08969118\n",
            "Iteration 1086, loss = 0.08965581\n",
            "Iteration 1087, loss = 0.08953259\n",
            "Iteration 1088, loss = 0.08938403\n",
            "Iteration 1089, loss = 0.08929835\n",
            "Iteration 1090, loss = 0.08922910\n",
            "Iteration 1091, loss = 0.08897836\n",
            "Iteration 1092, loss = 0.08888085\n",
            "Iteration 1093, loss = 0.08878891\n",
            "Iteration 1094, loss = 0.08868254\n",
            "Iteration 1095, loss = 0.08857055\n",
            "Iteration 1096, loss = 0.08843193\n",
            "Iteration 1097, loss = 0.08825784\n",
            "Iteration 1098, loss = 0.08819111\n",
            "Iteration 1099, loss = 0.08830169\n",
            "Iteration 1100, loss = 0.08795348\n",
            "Iteration 1101, loss = 0.08778951\n",
            "Iteration 1102, loss = 0.08767855\n",
            "Iteration 1103, loss = 0.08764791\n",
            "Iteration 1104, loss = 0.08755807\n",
            "Iteration 1105, loss = 0.08734528\n",
            "Iteration 1106, loss = 0.08744258\n",
            "Iteration 1107, loss = 0.08718983\n",
            "Iteration 1108, loss = 0.08703700\n",
            "Iteration 1109, loss = 0.08702513\n",
            "Iteration 1110, loss = 0.08687552\n",
            "Iteration 1111, loss = 0.08672261\n",
            "Iteration 1112, loss = 0.08652191\n",
            "Iteration 1113, loss = 0.08660843\n",
            "Iteration 1114, loss = 0.08658217\n",
            "Iteration 1115, loss = 0.08617082\n",
            "Iteration 1116, loss = 0.08606898\n",
            "Iteration 1117, loss = 0.08596776\n",
            "Iteration 1118, loss = 0.08588995\n",
            "Iteration 1119, loss = 0.08570822\n",
            "Iteration 1120, loss = 0.08571096\n",
            "Iteration 1121, loss = 0.08549626\n",
            "Iteration 1122, loss = 0.08537749\n",
            "Iteration 1123, loss = 0.08524345\n",
            "Iteration 1124, loss = 0.08533821\n",
            "Iteration 1125, loss = 0.08503601\n",
            "Iteration 1126, loss = 0.08502118\n",
            "Iteration 1127, loss = 0.08491056\n",
            "Iteration 1128, loss = 0.08472531\n",
            "Iteration 1129, loss = 0.08458941\n",
            "Iteration 1130, loss = 0.08465196\n",
            "Iteration 1131, loss = 0.08443349\n",
            "Iteration 1132, loss = 0.08437360\n",
            "Iteration 1133, loss = 0.08415360\n",
            "Iteration 1134, loss = 0.08427420\n",
            "Iteration 1135, loss = 0.08400240\n",
            "Iteration 1136, loss = 0.08384943\n",
            "Iteration 1137, loss = 0.08378382\n",
            "Iteration 1138, loss = 0.08367563\n",
            "Iteration 1139, loss = 0.08363769\n",
            "Iteration 1140, loss = 0.08346757\n",
            "Iteration 1141, loss = 0.08330793\n",
            "Iteration 1142, loss = 0.08328795\n",
            "Iteration 1143, loss = 0.08377380\n",
            "Iteration 1144, loss = 0.08372163\n",
            "Iteration 1145, loss = 0.08335582\n",
            "Iteration 1146, loss = 0.08282060\n",
            "Iteration 1147, loss = 0.08273109\n",
            "Iteration 1148, loss = 0.08282153\n",
            "Iteration 1149, loss = 0.08258600\n",
            "Iteration 1150, loss = 0.08259039\n",
            "Iteration 1151, loss = 0.08233938\n",
            "Iteration 1152, loss = 0.08227139\n",
            "Iteration 1153, loss = 0.08240793\n",
            "Iteration 1154, loss = 0.08201526\n",
            "Iteration 1155, loss = 0.08196184\n",
            "Iteration 1156, loss = 0.08233728\n",
            "Iteration 1157, loss = 0.08173468\n",
            "Iteration 1158, loss = 0.08156366\n",
            "Iteration 1159, loss = 0.08155878\n",
            "Iteration 1160, loss = 0.08152402\n",
            "Iteration 1161, loss = 0.08133378\n",
            "Iteration 1162, loss = 0.08119524\n",
            "Iteration 1163, loss = 0.08114270\n",
            "Iteration 1164, loss = 0.08112391\n",
            "Iteration 1165, loss = 0.08091767\n",
            "Iteration 1166, loss = 0.08079649\n",
            "Iteration 1167, loss = 0.08071300\n",
            "Iteration 1168, loss = 0.08054274\n",
            "Iteration 1169, loss = 0.08061336\n",
            "Iteration 1170, loss = 0.08047196\n",
            "Iteration 1171, loss = 0.08032097\n",
            "Iteration 1172, loss = 0.08015254\n",
            "Iteration 1173, loss = 0.08017175\n",
            "Iteration 1174, loss = 0.07993969\n",
            "Iteration 1175, loss = 0.07986567\n",
            "Iteration 1176, loss = 0.07980089\n",
            "Iteration 1177, loss = 0.07975234\n",
            "Iteration 1178, loss = 0.07988356\n",
            "Iteration 1179, loss = 0.07964674\n",
            "Iteration 1180, loss = 0.07970012\n",
            "Iteration 1181, loss = 0.07936816\n",
            "Iteration 1182, loss = 0.07924344\n",
            "Iteration 1183, loss = 0.07915053\n",
            "Iteration 1184, loss = 0.07908800\n",
            "Iteration 1185, loss = 0.07892449\n",
            "Iteration 1186, loss = 0.07897126\n",
            "Iteration 1187, loss = 0.07872882\n",
            "Iteration 1188, loss = 0.07866105\n",
            "Iteration 1189, loss = 0.07865117\n",
            "Iteration 1190, loss = 0.07849369\n",
            "Iteration 1191, loss = 0.07844504\n",
            "Iteration 1192, loss = 0.07838558\n",
            "Iteration 1193, loss = 0.07820272\n",
            "Iteration 1194, loss = 0.07820679\n",
            "Iteration 1195, loss = 0.07801360\n",
            "Iteration 1196, loss = 0.07793839\n",
            "Iteration 1197, loss = 0.07802459\n",
            "Iteration 1198, loss = 0.07774608\n",
            "Iteration 1199, loss = 0.07766959\n",
            "Iteration 1200, loss = 0.07758361\n",
            "Iteration 1201, loss = 0.07760108\n",
            "Iteration 1202, loss = 0.07750029\n",
            "Iteration 1203, loss = 0.07751224\n",
            "Iteration 1204, loss = 0.07721545\n",
            "Iteration 1205, loss = 0.07711883\n",
            "Iteration 1206, loss = 0.07703976\n",
            "Iteration 1207, loss = 0.07698358\n",
            "Iteration 1208, loss = 0.07688359\n",
            "Iteration 1209, loss = 0.07675410\n",
            "Iteration 1210, loss = 0.07671579\n",
            "Iteration 1211, loss = 0.07672800\n",
            "Iteration 1212, loss = 0.07685417\n",
            "Iteration 1213, loss = 0.07644788\n",
            "Iteration 1214, loss = 0.07640526\n",
            "Iteration 1215, loss = 0.07643243\n",
            "Iteration 1216, loss = 0.07615886\n",
            "Iteration 1217, loss = 0.07613594\n",
            "Iteration 1218, loss = 0.07628980\n",
            "Iteration 1219, loss = 0.07593571\n",
            "Iteration 1220, loss = 0.07579570\n",
            "Iteration 1221, loss = 0.07571774\n",
            "Iteration 1222, loss = 0.07580760\n",
            "Iteration 1223, loss = 0.07556360\n",
            "Iteration 1224, loss = 0.07548495\n",
            "Iteration 1225, loss = 0.07551430\n",
            "Iteration 1226, loss = 0.07540716\n",
            "Iteration 1227, loss = 0.07515972\n",
            "Iteration 1228, loss = 0.07516632\n",
            "Iteration 1229, loss = 0.07514967\n",
            "Iteration 1230, loss = 0.07500606\n",
            "Iteration 1231, loss = 0.07494416\n",
            "Iteration 1232, loss = 0.07500971\n",
            "Iteration 1233, loss = 0.07471213\n",
            "Iteration 1234, loss = 0.07459441\n",
            "Iteration 1235, loss = 0.07451574\n",
            "Iteration 1236, loss = 0.07450792\n",
            "Iteration 1237, loss = 0.07439632\n",
            "Iteration 1238, loss = 0.07435181\n",
            "Iteration 1239, loss = 0.07444752\n",
            "Iteration 1240, loss = 0.07411457\n",
            "Iteration 1241, loss = 0.07416965\n",
            "Iteration 1242, loss = 0.07416314\n",
            "Iteration 1243, loss = 0.07398394\n",
            "Iteration 1244, loss = 0.07392240\n",
            "Iteration 1245, loss = 0.07382163\n",
            "Iteration 1246, loss = 0.07364116\n",
            "Iteration 1247, loss = 0.07379071\n",
            "Iteration 1248, loss = 0.07347160\n",
            "Iteration 1249, loss = 0.07347528\n",
            "Iteration 1250, loss = 0.07334533\n",
            "Iteration 1251, loss = 0.07333056\n",
            "Iteration 1252, loss = 0.07330281\n",
            "Iteration 1253, loss = 0.07310686\n",
            "Iteration 1254, loss = 0.07296174\n",
            "Iteration 1255, loss = 0.07310792\n",
            "Iteration 1256, loss = 0.07293152\n",
            "Iteration 1257, loss = 0.07279384\n",
            "Iteration 1258, loss = 0.07275067\n",
            "Iteration 1259, loss = 0.07283219\n",
            "Iteration 1260, loss = 0.07249444\n",
            "Iteration 1261, loss = 0.07243749\n",
            "Iteration 1262, loss = 0.07238197\n",
            "Iteration 1263, loss = 0.07237841\n",
            "Iteration 1264, loss = 0.07229577\n",
            "Iteration 1265, loss = 0.07227925\n",
            "Iteration 1266, loss = 0.07242248\n",
            "Iteration 1267, loss = 0.07204448\n",
            "Iteration 1268, loss = 0.07198594\n",
            "Iteration 1269, loss = 0.07187094\n",
            "Iteration 1270, loss = 0.07216088\n",
            "Iteration 1271, loss = 0.07175146\n",
            "Iteration 1272, loss = 0.07172814\n",
            "Iteration 1273, loss = 0.07173192\n",
            "Iteration 1274, loss = 0.07167679\n",
            "Iteration 1275, loss = 0.07139421\n",
            "Iteration 1276, loss = 0.07148397\n",
            "Iteration 1277, loss = 0.07132405\n",
            "Iteration 1278, loss = 0.07121329\n",
            "Iteration 1279, loss = 0.07169316\n",
            "Iteration 1280, loss = 0.07110949\n",
            "Iteration 1281, loss = 0.07102512\n",
            "Iteration 1282, loss = 0.07111709\n",
            "Iteration 1283, loss = 0.07102126\n",
            "Iteration 1284, loss = 0.07084798\n",
            "Iteration 1285, loss = 0.07069157\n",
            "Iteration 1286, loss = 0.07081218\n",
            "Iteration 1287, loss = 0.07057653\n",
            "Iteration 1288, loss = 0.07047111\n",
            "Iteration 1289, loss = 0.07038599\n",
            "Iteration 1290, loss = 0.07040553\n",
            "Iteration 1291, loss = 0.07039079\n",
            "Iteration 1292, loss = 0.07020371\n",
            "Iteration 1293, loss = 0.07011947\n",
            "Iteration 1294, loss = 0.07011015\n",
            "Iteration 1295, loss = 0.06998531\n",
            "Iteration 1296, loss = 0.06990364\n",
            "Iteration 1297, loss = 0.06983985\n",
            "Iteration 1298, loss = 0.06978297\n",
            "Iteration 1299, loss = 0.06985441\n",
            "Iteration 1300, loss = 0.06965596\n",
            "Iteration 1301, loss = 0.06964887\n",
            "Iteration 1302, loss = 0.06949918\n",
            "Iteration 1303, loss = 0.06943559\n",
            "Iteration 1304, loss = 0.06968815\n",
            "Iteration 1305, loss = 0.06960149\n",
            "Iteration 1306, loss = 0.06923007\n",
            "Iteration 1307, loss = 0.06913449\n",
            "Iteration 1308, loss = 0.06917566\n",
            "Iteration 1309, loss = 0.06906601\n",
            "Iteration 1310, loss = 0.06892128\n",
            "Iteration 1311, loss = 0.06893756\n",
            "Iteration 1312, loss = 0.06885567\n",
            "Iteration 1313, loss = 0.06880967\n",
            "Iteration 1314, loss = 0.06864579\n",
            "Iteration 1315, loss = 0.06878773\n",
            "Iteration 1316, loss = 0.06855530\n",
            "Iteration 1317, loss = 0.06841894\n",
            "Iteration 1318, loss = 0.06836652\n",
            "Iteration 1319, loss = 0.06848016\n",
            "Iteration 1320, loss = 0.06828048\n",
            "Iteration 1321, loss = 0.06820082\n",
            "Iteration 1322, loss = 0.06853807\n",
            "Iteration 1323, loss = 0.06815386\n",
            "Iteration 1324, loss = 0.06816184\n",
            "Iteration 1325, loss = 0.06793109\n",
            "Iteration 1326, loss = 0.06815715\n",
            "Iteration 1327, loss = 0.06782799\n",
            "Iteration 1328, loss = 0.06784217\n",
            "Iteration 1329, loss = 0.06795344\n",
            "Iteration 1330, loss = 0.06762446\n",
            "Iteration 1331, loss = 0.06756673\n",
            "Iteration 1332, loss = 0.06747065\n",
            "Iteration 1333, loss = 0.06738093\n",
            "Iteration 1334, loss = 0.06750337\n",
            "Iteration 1335, loss = 0.06738417\n",
            "Iteration 1336, loss = 0.06751805\n",
            "Iteration 1337, loss = 0.06712851\n",
            "Iteration 1338, loss = 0.06720513\n",
            "Iteration 1339, loss = 0.06701066\n",
            "Iteration 1340, loss = 0.06713100\n",
            "Iteration 1341, loss = 0.06694902\n",
            "Iteration 1342, loss = 0.06695050\n",
            "Iteration 1343, loss = 0.06677623\n",
            "Iteration 1344, loss = 0.06675455\n",
            "Iteration 1345, loss = 0.06669131\n",
            "Iteration 1346, loss = 0.06674236\n",
            "Iteration 1347, loss = 0.06648591\n",
            "Iteration 1348, loss = 0.06652643\n",
            "Iteration 1349, loss = 0.06641744\n",
            "Iteration 1350, loss = 0.06650757\n",
            "Iteration 1351, loss = 0.06652924\n",
            "Iteration 1352, loss = 0.06618837\n",
            "Iteration 1353, loss = 0.06621363\n",
            "Iteration 1354, loss = 0.06613984\n",
            "Iteration 1355, loss = 0.06602687\n",
            "Iteration 1356, loss = 0.06617788\n",
            "Iteration 1357, loss = 0.06586942\n",
            "Iteration 1358, loss = 0.06596600\n",
            "Iteration 1359, loss = 0.06592027\n",
            "Iteration 1360, loss = 0.06584061\n",
            "Iteration 1361, loss = 0.06583200\n",
            "Iteration 1362, loss = 0.06571092\n",
            "Iteration 1363, loss = 0.06567999\n",
            "Iteration 1364, loss = 0.06561874\n",
            "Iteration 1365, loss = 0.06548972\n",
            "Iteration 1366, loss = 0.06536022\n",
            "Iteration 1367, loss = 0.06537241\n",
            "Iteration 1368, loss = 0.06538179\n",
            "Iteration 1369, loss = 0.06516681\n",
            "Iteration 1370, loss = 0.06510186\n",
            "Iteration 1371, loss = 0.06521903\n",
            "Iteration 1372, loss = 0.06516355\n",
            "Iteration 1373, loss = 0.06495794\n",
            "Iteration 1374, loss = 0.06495341\n",
            "Iteration 1375, loss = 0.06491242\n",
            "Iteration 1376, loss = 0.06478459\n",
            "Iteration 1377, loss = 0.06479208\n",
            "Iteration 1378, loss = 0.06478951\n",
            "Iteration 1379, loss = 0.06462363\n",
            "Iteration 1380, loss = 0.06461046\n",
            "Iteration 1381, loss = 0.06462975\n",
            "Iteration 1382, loss = 0.06454521\n",
            "Iteration 1383, loss = 0.06446966\n",
            "Iteration 1384, loss = 0.06430560\n",
            "Iteration 1385, loss = 0.06439767\n",
            "Iteration 1386, loss = 0.06420633\n",
            "Iteration 1387, loss = 0.06466727\n",
            "Iteration 1388, loss = 0.06407587\n",
            "Iteration 1389, loss = 0.06399177\n",
            "Iteration 1390, loss = 0.06396067\n",
            "Iteration 1391, loss = 0.06399240\n",
            "Iteration 1392, loss = 0.06432963\n",
            "Iteration 1393, loss = 0.06383286\n",
            "Iteration 1394, loss = 0.06377626\n",
            "Iteration 1395, loss = 0.06377897\n",
            "Iteration 1396, loss = 0.06379513\n",
            "Iteration 1397, loss = 0.06376897\n",
            "Iteration 1398, loss = 0.06349462\n",
            "Iteration 1399, loss = 0.06358545\n",
            "Iteration 1400, loss = 0.06354318\n",
            "Iteration 1401, loss = 0.06335035\n",
            "Iteration 1402, loss = 0.06336856\n",
            "Iteration 1403, loss = 0.06322777\n",
            "Iteration 1404, loss = 0.06321357\n",
            "Iteration 1405, loss = 0.06329437\n",
            "Iteration 1406, loss = 0.06340258\n",
            "Iteration 1407, loss = 0.06310570\n",
            "Iteration 1408, loss = 0.06305433\n",
            "Iteration 1409, loss = 0.06300001\n",
            "Iteration 1410, loss = 0.06319655\n",
            "Iteration 1411, loss = 0.06302047\n",
            "Iteration 1412, loss = 0.06274662\n",
            "Iteration 1413, loss = 0.06279255\n",
            "Iteration 1414, loss = 0.06318823\n",
            "Iteration 1415, loss = 0.06291470\n",
            "Iteration 1416, loss = 0.06269268\n",
            "Iteration 1417, loss = 0.06248370\n",
            "Iteration 1418, loss = 0.06251676\n",
            "Iteration 1419, loss = 0.06247457\n",
            "Iteration 1420, loss = 0.06241602\n",
            "Iteration 1421, loss = 0.06244625\n",
            "Iteration 1422, loss = 0.06230487\n",
            "Iteration 1423, loss = 0.06245068\n",
            "Iteration 1424, loss = 0.06209541\n",
            "Iteration 1425, loss = 0.06245835\n",
            "Iteration 1426, loss = 0.06236009\n",
            "Iteration 1427, loss = 0.06223064\n",
            "Iteration 1428, loss = 0.06204961\n",
            "Iteration 1429, loss = 0.06224620\n",
            "Iteration 1430, loss = 0.06195821\n",
            "Iteration 1431, loss = 0.06191936\n",
            "Iteration 1432, loss = 0.06184827\n",
            "Iteration 1433, loss = 0.06167318\n",
            "Iteration 1434, loss = 0.06188894\n",
            "Iteration 1435, loss = 0.06190145\n",
            "Iteration 1436, loss = 0.06157087\n",
            "Iteration 1437, loss = 0.06174348\n",
            "Iteration 1438, loss = 0.06186208\n",
            "Iteration 1439, loss = 0.06144396\n",
            "Iteration 1440, loss = 0.06128364\n",
            "Iteration 1441, loss = 0.06131410\n",
            "Iteration 1442, loss = 0.06123331\n",
            "Iteration 1443, loss = 0.06127105\n",
            "Iteration 1444, loss = 0.06116732\n",
            "Iteration 1445, loss = 0.06111805\n",
            "Iteration 1446, loss = 0.06111943\n",
            "Iteration 1447, loss = 0.06105372\n",
            "Iteration 1448, loss = 0.06167879\n",
            "Iteration 1449, loss = 0.06104086\n",
            "Iteration 1450, loss = 0.06098416\n",
            "Iteration 1451, loss = 0.06079309\n",
            "Iteration 1452, loss = 0.06079335\n",
            "Iteration 1453, loss = 0.06088528\n",
            "Iteration 1454, loss = 0.06059856\n",
            "Iteration 1455, loss = 0.06072487\n",
            "Iteration 1456, loss = 0.06085307\n",
            "Iteration 1457, loss = 0.06062987\n",
            "Iteration 1458, loss = 0.06064843\n",
            "Iteration 1459, loss = 0.06047594\n",
            "Iteration 1460, loss = 0.06036364\n",
            "Iteration 1461, loss = 0.06039552\n",
            "Iteration 1462, loss = 0.06029335\n",
            "Iteration 1463, loss = 0.06028939\n",
            "Iteration 1464, loss = 0.06020939\n",
            "Iteration 1465, loss = 0.06023517\n",
            "Iteration 1466, loss = 0.06021271\n",
            "Iteration 1467, loss = 0.06028377\n",
            "Iteration 1468, loss = 0.06013455\n",
            "Iteration 1469, loss = 0.06062784\n",
            "Iteration 1470, loss = 0.05999300\n",
            "Iteration 1471, loss = 0.05992964\n",
            "Iteration 1472, loss = 0.05976654\n",
            "Iteration 1473, loss = 0.05979144\n",
            "Iteration 1474, loss = 0.06009266\n",
            "Iteration 1475, loss = 0.05980266\n",
            "Iteration 1476, loss = 0.05964721\n",
            "Iteration 1477, loss = 0.05971734\n",
            "Iteration 1478, loss = 0.06017914\n",
            "Iteration 1479, loss = 0.05953437\n",
            "Iteration 1480, loss = 0.05970618\n",
            "Iteration 1481, loss = 0.05940167\n",
            "Iteration 1482, loss = 0.05943825\n",
            "Iteration 1483, loss = 0.05931758\n",
            "Iteration 1484, loss = 0.05932317\n",
            "Iteration 1485, loss = 0.05920700\n",
            "Iteration 1486, loss = 0.05923031\n",
            "Iteration 1487, loss = 0.05923848\n",
            "Iteration 1488, loss = 0.05921173\n",
            "Iteration 1489, loss = 0.05914993\n",
            "Iteration 1490, loss = 0.05899043\n",
            "Iteration 1491, loss = 0.05905393\n",
            "Iteration 1492, loss = 0.05893504\n",
            "Iteration 1493, loss = 0.05893702\n",
            "Iteration 1494, loss = 0.05882763\n",
            "Iteration 1495, loss = 0.05889239\n",
            "Iteration 1496, loss = 0.05875597\n",
            "Iteration 1497, loss = 0.05869767\n",
            "Iteration 1498, loss = 0.05867238\n",
            "Iteration 1499, loss = 0.05860557\n",
            "Iteration 1500, loss = 0.05857296\n",
            "Iteration 1501, loss = 0.05854199\n",
            "Iteration 1502, loss = 0.05853447\n",
            "Iteration 1503, loss = 0.05844659\n",
            "Iteration 1504, loss = 0.05872932\n",
            "Iteration 1505, loss = 0.05839461\n",
            "Iteration 1506, loss = 0.05864411\n",
            "Iteration 1507, loss = 0.05836586\n",
            "Iteration 1508, loss = 0.05822441\n",
            "Iteration 1509, loss = 0.05826324\n",
            "Iteration 1510, loss = 0.05852670\n",
            "Iteration 1511, loss = 0.05826014\n",
            "Iteration 1512, loss = 0.05808023\n",
            "Iteration 1513, loss = 0.05837341\n",
            "Iteration 1514, loss = 0.05797709\n",
            "Iteration 1515, loss = 0.05805968\n",
            "Iteration 1516, loss = 0.05810204\n",
            "Iteration 1517, loss = 0.05817996\n",
            "Iteration 1518, loss = 0.05787202\n",
            "Iteration 1519, loss = 0.05784168\n",
            "Iteration 1520, loss = 0.05792367\n",
            "Iteration 1521, loss = 0.05783324\n",
            "Iteration 1522, loss = 0.05771994\n",
            "Iteration 1523, loss = 0.05779673\n",
            "Iteration 1524, loss = 0.05771827\n",
            "Iteration 1525, loss = 0.05769278\n",
            "Iteration 1526, loss = 0.05749240\n",
            "Iteration 1527, loss = 0.05748383\n",
            "Iteration 1528, loss = 0.05761369\n",
            "Iteration 1529, loss = 0.05751549\n",
            "Iteration 1530, loss = 0.05739281\n",
            "Iteration 1531, loss = 0.05733673\n",
            "Iteration 1532, loss = 0.05725116\n",
            "Iteration 1533, loss = 0.05741662\n",
            "Iteration 1534, loss = 0.05723674\n",
            "Iteration 1535, loss = 0.05724448\n",
            "Iteration 1536, loss = 0.05715896\n",
            "Iteration 1537, loss = 0.05704447\n",
            "Iteration 1538, loss = 0.05742196\n",
            "Iteration 1539, loss = 0.05709978\n",
            "Iteration 1540, loss = 0.05692170\n",
            "Iteration 1541, loss = 0.05707125\n",
            "Iteration 1542, loss = 0.05689215\n",
            "Iteration 1543, loss = 0.05700809\n",
            "Iteration 1544, loss = 0.05681247\n",
            "Iteration 1545, loss = 0.05679649\n",
            "Iteration 1546, loss = 0.05676314\n",
            "Iteration 1547, loss = 0.05669928\n",
            "Iteration 1548, loss = 0.05663715\n",
            "Iteration 1549, loss = 0.05658151\n",
            "Iteration 1550, loss = 0.05656551\n",
            "Iteration 1551, loss = 0.05659537\n",
            "Iteration 1552, loss = 0.05665992\n",
            "Iteration 1553, loss = 0.05648909\n",
            "Iteration 1554, loss = 0.05680867\n",
            "Iteration 1555, loss = 0.05635512\n",
            "Iteration 1556, loss = 0.05632026\n",
            "Iteration 1557, loss = 0.05631678\n",
            "Iteration 1558, loss = 0.05619108\n",
            "Iteration 1559, loss = 0.05618841\n",
            "Iteration 1560, loss = 0.05614015\n",
            "Iteration 1561, loss = 0.05611265\n",
            "Iteration 1562, loss = 0.05630370\n",
            "Iteration 1563, loss = 0.05607515\n",
            "Iteration 1564, loss = 0.05605275\n",
            "Iteration 1565, loss = 0.05601985\n",
            "Iteration 1566, loss = 0.05589348\n",
            "Iteration 1567, loss = 0.05593372\n",
            "Iteration 1568, loss = 0.05585648\n",
            "Iteration 1569, loss = 0.05593700\n",
            "Iteration 1570, loss = 0.05583292\n",
            "Iteration 1571, loss = 0.05576801\n",
            "Iteration 1572, loss = 0.05577049\n",
            "Iteration 1573, loss = 0.05585045\n",
            "Iteration 1574, loss = 0.05577403\n",
            "Iteration 1575, loss = 0.05562658\n",
            "Iteration 1576, loss = 0.05569808\n",
            "Iteration 1577, loss = 0.05549546\n",
            "Iteration 1578, loss = 0.05545776\n",
            "Iteration 1579, loss = 0.05540072\n",
            "Iteration 1580, loss = 0.05583818\n",
            "Iteration 1581, loss = 0.05536670\n",
            "Iteration 1582, loss = 0.05537198\n",
            "Iteration 1583, loss = 0.05538766\n",
            "Iteration 1584, loss = 0.05525147\n",
            "Iteration 1585, loss = 0.05536825\n",
            "Iteration 1586, loss = 0.05510350\n",
            "Iteration 1587, loss = 0.05516862\n",
            "Iteration 1588, loss = 0.05520916\n",
            "Iteration 1589, loss = 0.05549734\n",
            "Iteration 1590, loss = 0.05525437\n",
            "Iteration 1591, loss = 0.05529903\n",
            "Iteration 1592, loss = 0.05512881\n",
            "Iteration 1593, loss = 0.05492017\n",
            "Iteration 1594, loss = 0.05491785\n",
            "Iteration 1595, loss = 0.05488253\n",
            "Iteration 1596, loss = 0.05507630\n",
            "Iteration 1597, loss = 0.05474028\n",
            "Iteration 1598, loss = 0.05474690\n",
            "Iteration 1599, loss = 0.05481626\n",
            "Iteration 1600, loss = 0.05503034\n",
            "Iteration 1601, loss = 0.05472935\n",
            "Iteration 1602, loss = 0.05466753\n",
            "Iteration 1603, loss = 0.05468310\n",
            "Iteration 1604, loss = 0.05456925\n",
            "Iteration 1605, loss = 0.05456158\n",
            "Iteration 1606, loss = 0.05471166\n",
            "Iteration 1607, loss = 0.05459504\n",
            "Iteration 1608, loss = 0.05439393\n",
            "Iteration 1609, loss = 0.05452063\n",
            "Iteration 1610, loss = 0.05456502\n",
            "Iteration 1611, loss = 0.05456016\n",
            "Iteration 1612, loss = 0.05453510\n",
            "Iteration 1613, loss = 0.05469294\n",
            "Iteration 1614, loss = 0.05426625\n",
            "Iteration 1615, loss = 0.05415577\n",
            "Iteration 1616, loss = 0.05451478\n",
            "Iteration 1617, loss = 0.05407971\n",
            "Iteration 1618, loss = 0.05409733\n",
            "Iteration 1619, loss = 0.05402137\n",
            "Iteration 1620, loss = 0.05417526\n",
            "Iteration 1621, loss = 0.05406538\n",
            "Iteration 1622, loss = 0.05414249\n",
            "Iteration 1623, loss = 0.05416740\n",
            "Iteration 1624, loss = 0.05383983\n",
            "Iteration 1625, loss = 0.05377090\n",
            "Iteration 1626, loss = 0.05408081\n",
            "Iteration 1627, loss = 0.05410095\n",
            "Iteration 1628, loss = 0.05398129\n",
            "Iteration 1629, loss = 0.05389292\n",
            "Iteration 1630, loss = 0.05387288\n",
            "Iteration 1631, loss = 0.05409188\n",
            "Iteration 1632, loss = 0.05437376\n",
            "Iteration 1633, loss = 0.05364872\n",
            "Iteration 1634, loss = 0.05386909\n",
            "Iteration 1635, loss = 0.05351706\n",
            "Iteration 1636, loss = 0.05351185\n",
            "Iteration 1637, loss = 0.05343627\n",
            "Iteration 1638, loss = 0.05344262\n",
            "Iteration 1639, loss = 0.05339357\n",
            "Iteration 1640, loss = 0.05336339\n",
            "Iteration 1641, loss = 0.05332514\n",
            "Iteration 1642, loss = 0.05330345\n",
            "Iteration 1643, loss = 0.05321526\n",
            "Iteration 1644, loss = 0.05371178\n",
            "Iteration 1645, loss = 0.05327594\n",
            "Iteration 1646, loss = 0.05340926\n",
            "Iteration 1647, loss = 0.05319149\n",
            "Iteration 1648, loss = 0.05314502\n",
            "Iteration 1649, loss = 0.05306144\n",
            "Iteration 1650, loss = 0.05324827\n",
            "Iteration 1651, loss = 0.05320569\n",
            "Iteration 1652, loss = 0.05300071\n",
            "Iteration 1653, loss = 0.05303075\n",
            "Iteration 1654, loss = 0.05298360\n",
            "Iteration 1655, loss = 0.05293943\n",
            "Iteration 1656, loss = 0.05289484\n",
            "Iteration 1657, loss = 0.05296047\n",
            "Iteration 1658, loss = 0.05302024\n",
            "Iteration 1659, loss = 0.05300047\n",
            "Iteration 1660, loss = 0.05279689\n",
            "Iteration 1661, loss = 0.05275307\n",
            "Iteration 1662, loss = 0.05269339\n",
            "Iteration 1663, loss = 0.05275266\n",
            "Iteration 1664, loss = 0.05261170\n",
            "Iteration 1665, loss = 0.05266107\n",
            "Iteration 1666, loss = 0.05276532\n",
            "Iteration 1667, loss = 0.05274387\n",
            "Iteration 1668, loss = 0.05248737\n",
            "Iteration 1669, loss = 0.05271849\n",
            "Iteration 1670, loss = 0.05252996\n",
            "Iteration 1671, loss = 0.05238561\n",
            "Iteration 1672, loss = 0.05233976\n",
            "Iteration 1673, loss = 0.05235885\n",
            "Iteration 1674, loss = 0.05290174\n",
            "Iteration 1675, loss = 0.05252105\n",
            "Iteration 1676, loss = 0.05234067\n",
            "Iteration 1677, loss = 0.05224051\n",
            "Iteration 1678, loss = 0.05215324\n",
            "Iteration 1679, loss = 0.05222050\n",
            "Iteration 1680, loss = 0.05233768\n",
            "Iteration 1681, loss = 0.05221977\n",
            "Iteration 1682, loss = 0.05226246\n",
            "Iteration 1683, loss = 0.05200328\n",
            "Iteration 1684, loss = 0.05227101\n",
            "Iteration 1685, loss = 0.05229071\n",
            "Iteration 1686, loss = 0.05214644\n",
            "Iteration 1687, loss = 0.05206042\n",
            "Iteration 1688, loss = 0.05189729\n",
            "Iteration 1689, loss = 0.05206215\n",
            "Iteration 1690, loss = 0.05186457\n",
            "Iteration 1691, loss = 0.05212339\n",
            "Iteration 1692, loss = 0.05193622\n",
            "Iteration 1693, loss = 0.05199151\n",
            "Iteration 1694, loss = 0.05188418\n",
            "Iteration 1695, loss = 0.05170390\n",
            "Iteration 1696, loss = 0.05169956\n",
            "Iteration 1697, loss = 0.05170235\n",
            "Iteration 1698, loss = 0.05184082\n",
            "Iteration 1699, loss = 0.05164374\n",
            "Iteration 1700, loss = 0.05163696\n",
            "Iteration 1701, loss = 0.05158328\n",
            "Iteration 1702, loss = 0.05164820\n",
            "Iteration 1703, loss = 0.05187097\n",
            "Iteration 1704, loss = 0.05148071\n",
            "Iteration 1705, loss = 0.05142530\n",
            "Iteration 1706, loss = 0.05156306\n",
            "Iteration 1707, loss = 0.05140525\n",
            "Iteration 1708, loss = 0.05129133\n",
            "Iteration 1709, loss = 0.05129013\n",
            "Iteration 1710, loss = 0.05140609\n",
            "Iteration 1711, loss = 0.05128825\n",
            "Iteration 1712, loss = 0.05129421\n",
            "Iteration 1713, loss = 0.05132268\n",
            "Iteration 1714, loss = 0.05126492\n",
            "Iteration 1715, loss = 0.05124532\n",
            "Iteration 1716, loss = 0.05105734\n",
            "Iteration 1717, loss = 0.05133080\n",
            "Iteration 1718, loss = 0.05113142\n",
            "Iteration 1719, loss = 0.05112094\n",
            "Iteration 1720, loss = 0.05107196\n",
            "Iteration 1721, loss = 0.05105619\n",
            "Iteration 1722, loss = 0.05102783\n",
            "Iteration 1723, loss = 0.05098879\n",
            "Iteration 1724, loss = 0.05109532\n",
            "Iteration 1725, loss = 0.05089452\n",
            "Iteration 1726, loss = 0.05084742\n",
            "Iteration 1727, loss = 0.05088947\n",
            "Iteration 1728, loss = 0.05110509\n",
            "Iteration 1729, loss = 0.05095625\n",
            "Iteration 1730, loss = 0.05071364\n",
            "Iteration 1731, loss = 0.05068609\n",
            "Iteration 1732, loss = 0.05064457\n",
            "Iteration 1733, loss = 0.05073188\n",
            "Iteration 1734, loss = 0.05066841\n",
            "Iteration 1735, loss = 0.05068971\n",
            "Iteration 1736, loss = 0.05069190\n",
            "Iteration 1737, loss = 0.05052685\n",
            "Iteration 1738, loss = 0.05055928\n",
            "Iteration 1739, loss = 0.05067885\n",
            "Iteration 1740, loss = 0.05054057\n",
            "Iteration 1741, loss = 0.05039639\n",
            "Iteration 1742, loss = 0.05049065\n",
            "Iteration 1743, loss = 0.05044182\n",
            "Iteration 1744, loss = 0.05045223\n",
            "Iteration 1745, loss = 0.05030433\n",
            "Iteration 1746, loss = 0.05027162\n",
            "Iteration 1747, loss = 0.05037690\n",
            "Iteration 1748, loss = 0.05022100\n",
            "Iteration 1749, loss = 0.05027119\n",
            "Iteration 1750, loss = 0.05033129\n",
            "Iteration 1751, loss = 0.05022843\n",
            "Iteration 1752, loss = 0.05048263\n",
            "Iteration 1753, loss = 0.05024458\n",
            "Iteration 1754, loss = 0.05012005\n",
            "Iteration 1755, loss = 0.05021612\n",
            "Iteration 1756, loss = 0.05022731\n",
            "Iteration 1757, loss = 0.05012509\n",
            "Iteration 1758, loss = 0.05010644\n",
            "Iteration 1759, loss = 0.05000246\n",
            "Iteration 1760, loss = 0.04997591\n",
            "Iteration 1761, loss = 0.04993829\n",
            "Iteration 1762, loss = 0.04995380\n",
            "Iteration 1763, loss = 0.05000885\n",
            "Iteration 1764, loss = 0.04977707\n",
            "Iteration 1765, loss = 0.04981315\n",
            "Iteration 1766, loss = 0.04988673\n",
            "Iteration 1767, loss = 0.04977755\n",
            "Iteration 1768, loss = 0.04984217\n",
            "Iteration 1769, loss = 0.05004707\n",
            "Iteration 1770, loss = 0.04980924\n",
            "Iteration 1771, loss = 0.04972698\n",
            "Iteration 1772, loss = 0.04976062\n",
            "Iteration 1773, loss = 0.04964255\n",
            "Iteration 1774, loss = 0.04972303\n",
            "Iteration 1775, loss = 0.04956955\n",
            "Iteration 1776, loss = 0.04967910\n",
            "Iteration 1777, loss = 0.04956233\n",
            "Iteration 1778, loss = 0.04983725\n",
            "Iteration 1779, loss = 0.04948495\n",
            "Iteration 1780, loss = 0.04977550\n",
            "Iteration 1781, loss = 0.04946947\n",
            "Iteration 1782, loss = 0.04942459\n",
            "Iteration 1783, loss = 0.04937151\n",
            "Iteration 1784, loss = 0.04945126\n",
            "Iteration 1785, loss = 0.04936286\n",
            "Iteration 1786, loss = 0.04936861\n",
            "Iteration 1787, loss = 0.04945957\n",
            "Iteration 1788, loss = 0.04952960\n",
            "Iteration 1789, loss = 0.04922011\n",
            "Iteration 1790, loss = 0.04920068\n",
            "Iteration 1791, loss = 0.04918925\n",
            "Iteration 1792, loss = 0.04932310\n",
            "Iteration 1793, loss = 0.04911263\n",
            "Iteration 1794, loss = 0.04905005\n",
            "Iteration 1795, loss = 0.04927048\n",
            "Iteration 1796, loss = 0.04904291\n",
            "Iteration 1797, loss = 0.04914974\n",
            "Iteration 1798, loss = 0.04945631\n",
            "Iteration 1799, loss = 0.04925119\n",
            "Iteration 1800, loss = 0.04891613\n",
            "Iteration 1801, loss = 0.04904475\n",
            "Iteration 1802, loss = 0.04896715\n",
            "Iteration 1803, loss = 0.04933698\n",
            "Iteration 1804, loss = 0.04884275\n",
            "Iteration 1805, loss = 0.04885931\n",
            "Iteration 1806, loss = 0.04896403\n",
            "Iteration 1807, loss = 0.04905600\n",
            "Iteration 1808, loss = 0.04880816\n",
            "Iteration 1809, loss = 0.04886680\n",
            "Iteration 1810, loss = 0.04874711\n",
            "Iteration 1811, loss = 0.04874568\n",
            "Iteration 1812, loss = 0.04876680\n",
            "Iteration 1813, loss = 0.04872658\n",
            "Iteration 1814, loss = 0.04873393\n",
            "Iteration 1815, loss = 0.04862987\n",
            "Iteration 1816, loss = 0.04869288\n",
            "Iteration 1817, loss = 0.04856631\n",
            "Iteration 1818, loss = 0.04862345\n",
            "Iteration 1819, loss = 0.04860134\n",
            "Iteration 1820, loss = 0.04858564\n",
            "Iteration 1821, loss = 0.04863420\n",
            "Iteration 1822, loss = 0.04836684\n",
            "Iteration 1823, loss = 0.04838747\n",
            "Iteration 1824, loss = 0.04851889\n",
            "Iteration 1825, loss = 0.04840159\n",
            "Iteration 1826, loss = 0.04848870\n",
            "Iteration 1827, loss = 0.04833393\n",
            "Iteration 1828, loss = 0.04846402\n",
            "Iteration 1829, loss = 0.04831297\n",
            "Iteration 1830, loss = 0.04878830\n",
            "Iteration 1831, loss = 0.04831680\n",
            "Iteration 1832, loss = 0.04835172\n",
            "Iteration 1833, loss = 0.04822637\n",
            "Iteration 1834, loss = 0.04819487\n",
            "Iteration 1835, loss = 0.04825248\n",
            "Iteration 1836, loss = 0.04933040\n",
            "Iteration 1837, loss = 0.04837537\n",
            "Iteration 1838, loss = 0.04862176\n",
            "Iteration 1839, loss = 0.04801567\n",
            "Iteration 1840, loss = 0.04808071\n",
            "Iteration 1841, loss = 0.04807732\n",
            "Iteration 1842, loss = 0.04807019\n",
            "Iteration 1843, loss = 0.04803632\n",
            "Iteration 1844, loss = 0.04819875\n",
            "Iteration 1845, loss = 0.04804166\n",
            "Iteration 1846, loss = 0.04805327\n",
            "Iteration 1847, loss = 0.04806171\n",
            "Iteration 1848, loss = 0.04839351\n",
            "Iteration 1849, loss = 0.04796223\n",
            "Iteration 1850, loss = 0.04780346\n",
            "Iteration 1851, loss = 0.04805395\n",
            "Iteration 1852, loss = 0.04780437\n",
            "Iteration 1853, loss = 0.04804134\n",
            "Iteration 1854, loss = 0.04774761\n",
            "Iteration 1855, loss = 0.04789641\n",
            "Iteration 1856, loss = 0.04785382\n",
            "Iteration 1857, loss = 0.04765723\n",
            "Iteration 1858, loss = 0.04802747\n",
            "Iteration 1859, loss = 0.04761819\n",
            "Iteration 1860, loss = 0.04762634\n",
            "Iteration 1861, loss = 0.04772032\n",
            "Iteration 1862, loss = 0.04772314\n",
            "Iteration 1863, loss = 0.04783585\n",
            "Iteration 1864, loss = 0.04765176\n",
            "Iteration 1865, loss = 0.04747505\n",
            "Iteration 1866, loss = 0.04760047\n",
            "Iteration 1867, loss = 0.04797922\n",
            "Iteration 1868, loss = 0.04760013\n",
            "Iteration 1869, loss = 0.04765052\n",
            "Iteration 1870, loss = 0.04761655\n",
            "Iteration 1871, loss = 0.04748645\n",
            "Iteration 1872, loss = 0.04751328\n",
            "Iteration 1873, loss = 0.04736149\n",
            "Iteration 1874, loss = 0.04735151\n",
            "Iteration 1875, loss = 0.04729901\n",
            "Iteration 1876, loss = 0.04734519\n",
            "Iteration 1877, loss = 0.04734223\n",
            "Iteration 1878, loss = 0.04726870\n",
            "Iteration 1879, loss = 0.04783044\n",
            "Iteration 1880, loss = 0.04720409\n",
            "Iteration 1881, loss = 0.04745571\n",
            "Iteration 1882, loss = 0.04730943\n",
            "Iteration 1883, loss = 0.04721596\n",
            "Iteration 1884, loss = 0.04727490\n",
            "Iteration 1885, loss = 0.04707916\n",
            "Iteration 1886, loss = 0.04705873\n",
            "Iteration 1887, loss = 0.04719928\n",
            "Iteration 1888, loss = 0.04709758\n",
            "Iteration 1889, loss = 0.04716924\n",
            "Iteration 1890, loss = 0.04703121\n",
            "Iteration 1891, loss = 0.04696037\n",
            "Iteration 1892, loss = 0.04709005\n",
            "Iteration 1893, loss = 0.04698232\n",
            "Iteration 1894, loss = 0.04693677\n",
            "Iteration 1895, loss = 0.04700896\n",
            "Iteration 1896, loss = 0.04689990\n",
            "Iteration 1897, loss = 0.04715015\n",
            "Iteration 1898, loss = 0.04686608\n",
            "Iteration 1899, loss = 0.04685752\n",
            "Iteration 1900, loss = 0.04678268\n",
            "Iteration 1901, loss = 0.04699200\n",
            "Iteration 1902, loss = 0.04677570\n",
            "Iteration 1903, loss = 0.04694804\n",
            "Iteration 1904, loss = 0.04686965\n",
            "Iteration 1905, loss = 0.04668731\n",
            "Iteration 1906, loss = 0.04687604\n",
            "Iteration 1907, loss = 0.04661854\n",
            "Iteration 1908, loss = 0.04671720\n",
            "Iteration 1909, loss = 0.04672020\n",
            "Iteration 1910, loss = 0.04681586\n",
            "Iteration 1911, loss = 0.04666822\n",
            "Iteration 1912, loss = 0.04672692\n",
            "Iteration 1913, loss = 0.04659632\n",
            "Iteration 1914, loss = 0.04683439\n",
            "Iteration 1915, loss = 0.04654477\n",
            "Iteration 1916, loss = 0.04685361\n",
            "Iteration 1917, loss = 0.04662077\n",
            "Iteration 1918, loss = 0.04656135\n",
            "Iteration 1919, loss = 0.04645785\n",
            "Iteration 1920, loss = 0.04644969\n",
            "Iteration 1921, loss = 0.04638340\n",
            "Iteration 1922, loss = 0.04647185\n",
            "Iteration 1923, loss = 0.04635631\n",
            "Iteration 1924, loss = 0.04641191\n",
            "Iteration 1925, loss = 0.04633638\n",
            "Iteration 1926, loss = 0.04633713\n",
            "Iteration 1927, loss = 0.04670332\n",
            "Iteration 1928, loss = 0.04631473\n",
            "Iteration 1929, loss = 0.04641816\n",
            "Iteration 1930, loss = 0.04627343\n",
            "Iteration 1931, loss = 0.04636420\n",
            "Iteration 1932, loss = 0.04635912\n",
            "Iteration 1933, loss = 0.04617591\n",
            "Iteration 1934, loss = 0.04637546\n",
            "Iteration 1935, loss = 0.04657391\n",
            "Iteration 1936, loss = 0.04613652\n",
            "Iteration 1937, loss = 0.04611191\n",
            "Iteration 1938, loss = 0.04611780\n",
            "Iteration 1939, loss = 0.04640477\n",
            "Iteration 1940, loss = 0.04612608\n",
            "Iteration 1941, loss = 0.04621581\n",
            "Iteration 1942, loss = 0.04609516\n",
            "Iteration 1943, loss = 0.04599429\n",
            "Iteration 1944, loss = 0.04603414\n",
            "Iteration 1945, loss = 0.04622718\n",
            "Iteration 1946, loss = 0.04603578\n",
            "Iteration 1947, loss = 0.04610352\n",
            "Iteration 1948, loss = 0.04612777\n",
            "Iteration 1949, loss = 0.04590720\n",
            "Iteration 1950, loss = 0.04589268\n",
            "Iteration 1951, loss = 0.04602710\n",
            "Iteration 1952, loss = 0.04601916\n",
            "Iteration 1953, loss = 0.04581388\n",
            "Iteration 1954, loss = 0.04579103\n",
            "Iteration 1955, loss = 0.04592123\n",
            "Iteration 1956, loss = 0.04605616\n",
            "Iteration 1957, loss = 0.04577809\n",
            "Iteration 1958, loss = 0.04608943\n",
            "Iteration 1959, loss = 0.04590820\n",
            "Iteration 1960, loss = 0.04569975\n",
            "Iteration 1961, loss = 0.04568342\n",
            "Iteration 1962, loss = 0.04584646\n",
            "Iteration 1963, loss = 0.04569886\n",
            "Iteration 1964, loss = 0.04572633\n",
            "Iteration 1965, loss = 0.04569841\n",
            "Iteration 1966, loss = 0.04566859\n",
            "Iteration 1967, loss = 0.04569347\n",
            "Iteration 1968, loss = 0.04598468\n",
            "Iteration 1969, loss = 0.04579301\n",
            "Iteration 1970, loss = 0.04551798\n",
            "Iteration 1971, loss = 0.04564577\n",
            "Iteration 1972, loss = 0.04545452\n",
            "Iteration 1973, loss = 0.04592301\n",
            "Iteration 1974, loss = 0.04544922\n",
            "Iteration 1975, loss = 0.04552555\n",
            "Iteration 1976, loss = 0.04566534\n",
            "Iteration 1977, loss = 0.04548086\n",
            "Iteration 1978, loss = 0.04544261\n",
            "Iteration 1979, loss = 0.04529375\n",
            "Iteration 1980, loss = 0.04532900\n",
            "Iteration 1981, loss = 0.04561978\n",
            "Iteration 1982, loss = 0.04534185\n",
            "Iteration 1983, loss = 0.04527790\n",
            "Iteration 1984, loss = 0.04539694\n",
            "Iteration 1985, loss = 0.04560247\n",
            "Iteration 1986, loss = 0.04534445\n",
            "Iteration 1987, loss = 0.04528474\n",
            "Iteration 1988, loss = 0.04531894\n",
            "Iteration 1989, loss = 0.04538256\n",
            "Iteration 1990, loss = 0.04542643\n",
            "Iteration 1991, loss = 0.04531656\n",
            "Iteration 1992, loss = 0.04554372\n",
            "Iteration 1993, loss = 0.04517494\n",
            "Iteration 1994, loss = 0.04535347\n",
            "Iteration 1995, loss = 0.04540861\n",
            "Iteration 1996, loss = 0.04520065\n",
            "Iteration 1997, loss = 0.04511994\n",
            "Iteration 1998, loss = 0.04513359\n",
            "Iteration 1999, loss = 0.04509471\n",
            "Iteration 2000, loss = 0.04523106\n",
            "Iteration 2001, loss = 0.04511315\n",
            "Iteration 2002, loss = 0.04496132\n",
            "Iteration 2003, loss = 0.04503699\n",
            "Iteration 2004, loss = 0.04516002\n",
            "Iteration 2005, loss = 0.04503487\n",
            "Iteration 2006, loss = 0.04493285\n",
            "Iteration 2007, loss = 0.04489534\n",
            "Iteration 2008, loss = 0.04502469\n",
            "Iteration 2009, loss = 0.04539471\n",
            "Iteration 2010, loss = 0.04491715\n",
            "Iteration 2011, loss = 0.04481189\n",
            "Iteration 2012, loss = 0.04483263\n",
            "Iteration 2013, loss = 0.04478471\n",
            "Iteration 2014, loss = 0.04477843\n",
            "Iteration 2015, loss = 0.04477505\n",
            "Iteration 2016, loss = 0.04476654\n",
            "Iteration 2017, loss = 0.04471641\n",
            "Iteration 2018, loss = 0.04487522\n",
            "Iteration 2019, loss = 0.04484278\n",
            "Iteration 2020, loss = 0.04471305\n",
            "Iteration 2021, loss = 0.04468168\n",
            "Iteration 2022, loss = 0.04511477\n",
            "Iteration 2023, loss = 0.04475441\n",
            "Iteration 2024, loss = 0.04469434\n",
            "Iteration 2025, loss = 0.04466253\n",
            "Iteration 2026, loss = 0.04458565\n",
            "Iteration 2027, loss = 0.04477941\n",
            "Iteration 2028, loss = 0.04467106\n",
            "Iteration 2029, loss = 0.04453855\n",
            "Iteration 2030, loss = 0.04460971\n",
            "Iteration 2031, loss = 0.04456267\n",
            "Iteration 2032, loss = 0.04488075\n",
            "Iteration 2033, loss = 0.04475362\n",
            "Iteration 2034, loss = 0.04451509\n",
            "Iteration 2035, loss = 0.04448090\n",
            "Iteration 2036, loss = 0.04447246\n",
            "Iteration 2037, loss = 0.04486215\n",
            "Iteration 2038, loss = 0.04455603\n",
            "Iteration 2039, loss = 0.04477867\n",
            "Iteration 2040, loss = 0.04440457\n",
            "Iteration 2041, loss = 0.04493737\n",
            "Iteration 2042, loss = 0.04433627\n",
            "Iteration 2043, loss = 0.04432917\n",
            "Iteration 2044, loss = 0.04441358\n",
            "Iteration 2045, loss = 0.04436484\n",
            "Iteration 2046, loss = 0.04452235\n",
            "Iteration 2047, loss = 0.04431651\n",
            "Iteration 2048, loss = 0.04421722\n",
            "Iteration 2049, loss = 0.04419496\n",
            "Iteration 2050, loss = 0.04431113\n",
            "Iteration 2051, loss = 0.04421898\n",
            "Iteration 2052, loss = 0.04436415\n",
            "Iteration 2053, loss = 0.04445381\n",
            "Iteration 2054, loss = 0.04430497\n",
            "Iteration 2055, loss = 0.04409959\n",
            "Iteration 2056, loss = 0.04425631\n",
            "Iteration 2057, loss = 0.04423934\n",
            "Iteration 2058, loss = 0.04441626\n",
            "Iteration 2059, loss = 0.04474126\n",
            "Iteration 2060, loss = 0.04418335\n",
            "Iteration 2061, loss = 0.04427268\n",
            "Iteration 2062, loss = 0.04411502\n",
            "Iteration 2063, loss = 0.04425280\n",
            "Iteration 2064, loss = 0.04417615\n",
            "Iteration 2065, loss = 0.04426179\n",
            "Iteration 2066, loss = 0.04413456\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', batch_size=32, hidden_layer_sizes=(4, 4),\n",
              "              max_iter=3000, tol=1e-05, verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, batch_size=32, hidden_layer_sizes=(4, 4),\n",
              "              max_iter=3000, tol=1e-05, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, batch_size=32, hidden_layer_sizes=(4, 4),\n",
              "              max_iter=3000, tol=1e-05, verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network.classes_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bID-SbNCGWle",
        "outputId": "d3a1a94e-dd6a-4adf-bb21-00511d9173f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(network.coefs_) #uses bias units by default\n",
        "#related to bias is\n",
        "print(network.intercepts_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULOttdSGGZp_",
        "outputId": "53f5938c-a185-421d-abac-722274e386a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[ 0.61541992, -0.07029332,  0.82838104, -0.51322894],\n",
            "       [ 1.18272858, -1.96862363,  1.11335019, -1.05639468],\n",
            "       [-1.35866696,  2.14293805, -1.36308622,  1.14533998],\n",
            "       [-2.01186527,  2.10518597, -2.29900489,  1.83404754]]), array([[ 3.91828347, -3.62652294, -2.20593031, -1.68713557],\n",
            "       [-1.38754541,  1.23384314,  4.98783684,  4.98410975],\n",
            "       [ 3.55030269, -3.6427958 , -2.02738097, -1.8785996 ],\n",
            "       [-2.48276786,  2.61756702,  2.90387082,  3.26695107]]), array([[ 3.78840695,  2.12013689, -4.38981979],\n",
            "       [-2.11835502, -2.75011068,  3.48032131],\n",
            "       [-4.43811224,  1.46067268,  1.13727449],\n",
            "       [-4.31831711,  1.67593738,  1.29278464]])]\n",
            "[array([ 2.42801625, -0.82643583,  1.56622769, -1.86083296]), array([ 0.44613209, -0.49350953,  0.58418212, -0.06233926]), array([ 3.03675567, -0.89829227, -0.12655782])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(network.n_layers_)\n",
        "print(network.n_outputs_)\n",
        "print(network.out_activation_) #classification so automatic softmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8B1kJm_Gnpj",
        "outputId": "da5a79f9-bbad-4f8d-b030-6c82e325a7cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "3\n",
            "softmax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = network.predict(x_test)\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxsF7NT9HfWO",
        "outputId": "b0eccc8d-1d9b-43f0-87c5-9704c4cf0792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 2, 0, 2, 1, 1, 1, 0, 2, 2, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 0,\n",
              "       0, 1, 0, 0, 1, 1, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRQ2gpiuHpwR",
        "outputId": "aca6b4dc-c91b-4994-9148-6094b7f605b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 2, 0, 2, 1, 1, 1, 0, 2, 2, 0, 2, 2, 0, 2, 0, 1, 2, 0, 2, 0,\n",
              "       0, 1, 0, 0, 1, 1, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "print(accuracy_score(y_test, predictions)) #93% accuracy\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "cm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVxhaXi-Hxy4",
        "outputId": "78fd0f2c-84b9-478e-bbc4-07131d416a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9333333333333333\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[12,  0,  0],\n",
              "       [ 0,  8,  0],\n",
              "       [ 0,  2,  8]])"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[0], y_test[0]\n",
        "new = x_test[0].reshape(1,-1)\n",
        "new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_VIlpRAIJV_",
        "outputId": "390c5566-b812-48e1-d949-ad4df7122e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.8, 3.1, 1.6, 0.2]])"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network.predict(new)#correct prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lq3FCIWIYuW",
        "outputId": "a5c4b8af-59ee-4f84-b899-41d0cc4f1f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Searching"
      ],
      "metadata": {
        "id": "rhSBPzmUK1IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "K8QsHEMAK3Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    \"max_iter\" : [1000,2000,5000],\n",
        "    \"activation\" : [\"logistic\", \"tanh\"],\n",
        "    \"solver\" : [\"adam\", \"sgd\"],\n",
        "    \"batch_size\" : [5,12]\n",
        "}\n",
        "\n",
        "#parameters is a dictionary storing a list of the parameters and the values to test out, this\n",
        "#gets put into the network and it will automatically chose the best parameters from here"
      ],
      "metadata": {
        "id": "UnAjJo-JK-Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(estimator = MLPClassifier(), param_grid=parameters) #test on a MLP network\n",
        "grid_search.fit(inputs, outputs)\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "print(best_params, best_score) #final row shows the best epoch count, activation function, optimizer, and batch size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR8ms-WTLeKt",
        "outputId": "6b36a06d-7a83-472b-fe6a-9c46b08e88e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'activation': 'tanh', 'batch_size': 5, 'max_iter': 2000, 'solver': 'sgd'} 0.9933333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorFlow Network"
      ],
      "metadata": {
        "id": "jA5v6ki_Lt1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Googles network library; is used for and supports deep learning, as well as GPU access for faster processing time"
      ],
      "metadata": {
        "id": "QD8BRxMLPK-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow"
      ],
      "metadata": {
        "id": "YyqTMxvcL2-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorflow.__version__ #want ver 2.0 +"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YqWkyNBHPg_8",
        "outputId": "e6c1f42e-d5ea-4a1c-855d-25a6e1d11027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential #class used to create NN\n",
        "from tensorflow.keras.layers import Dense #same as fullconnection in pybrain, connect neuron to entire next layer\n",
        "from keras.utils import np_utils #numpy utilities\n",
        "from tensorflow.keras.datasets import mnist #mnist data"
      ],
      "metadata": {
        "id": "i3cnfcTRPltp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTmwuTdsQR6m",
        "outputId": "e7981694-9e1e-45ce-9e30-76e4f4937309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_train[0])\n",
        "plt.title(\"Class: \" +str(Y_train[0])) #see the images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "ouGtOYntQfi1",
        "outputId": "f86df82a-917a-4949-c204-427354728d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Class: 5')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape #60,000 images, with 28 pixels wide and 28 pixels high, so 784 pixels per image\n",
        "X_train[0] #looking at the input by itself, first image is 28 matricies with 28 inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fGpSMdqRqiy",
        "outputId": "69a9eb4c-a444-4ad4-c99c-4e91155fdf9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(60000, 28*28)\n",
        "X_train.shape #now only have 1 direction so network input is composed of 784 neurons\n",
        "X_train[0] #see the actual pixel makeup closer to 0 the darker, the closer to 255 the brighter\n",
        "#now the input is just 1 matrix of 784"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BOfsMHUSKJl",
        "outputId": "d42f9a4b-b515-4231-81ef-4866fdb936e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
              "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
              "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
              "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
              "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
              "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
              "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
              "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
              "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
              "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test.reshape(10000, 28*28)\n",
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb55VRVcTK3B",
        "outputId": "6aef9e80-64db-437d-f322-0e56d1b167ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#want to normalize to make the processesing of data faster\n",
        "#change to float first\n",
        "X_train = X_train.astype(\"float32\")\n",
        "X_test = X_test.astype(\"float32\")\n",
        "\n",
        "#divide by 255 to normalize\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "X_train.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKw1-OMiTWwL",
        "outputId": "e597b3f6-79fe-4b45-f15a-0493dae53c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Y_train is correct response for each X_train\n",
        "Y_train\n",
        "#since we have 0-9 numbers to identify we'll want 10 neurons in the output layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Di-K0dYUEpw",
        "outputId": "e3e27a32-b76d-4b52-ef6f-68362d930d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = np_utils.to_categorical(Y_train)\n",
        "Y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYnNbT6DUYib",
        "outputId": "cd1f3b22-e96d-45d9-cddc-496b5562bae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test = np_utils.to_categorical(Y_test)\n"
      ],
      "metadata": {
        "id": "hwj9xVzMUob_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network Training"
      ],
      "metadata": {
        "id": "SRKhABf_XXk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(784 + 10)/2\n",
        "\n",
        "#397 neurons in hidden layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBAl3NGzXgqi",
        "outputId": "e6817931-94f2-4ca9-f8bf-eebc6fcd3ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "397.0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#structure is: 784 -> 397 (hidden) -> 397 (hidden) -> 10 (output)\n",
        "network = Sequential()\n",
        "network.add(Dense(input_shape = (784,), units = 397, activation =\"relu\"))\n",
        "network.add(Dense(units = 397, activation =\"relu\"))\n",
        "network.add(Dense(units = 10, activation =\"softmax\"))"
      ],
      "metadata": {
        "id": "_TSroROrXaNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network.compile(loss = \"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"]) #loss functions shown in keras/tensorflow documentation # adam is stochastic gradient descent method"
      ],
      "metadata": {
        "id": "KU2pcJ4pYcon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = network.fit(X_train, Y_train, batch_size = 128, epochs = 10) #batch size is that \"k fold gradient descent\"\n",
        "#messed up cause y output is bad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2jQz5ltZDBW",
        "outputId": "02719c0f-3375-4de6-8bce-5cd79f17e574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.2251 - accuracy: 0.9337\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0824 - accuracy: 0.9751\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0546 - accuracy: 0.9829\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0381 - accuracy: 0.9879\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0264 - accuracy: 0.9916\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0219 - accuracy: 0.9928\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0203 - accuracy: 0.9932\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0175 - accuracy: 0.9941\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0129 - accuracy: 0.9955\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0131 - accuracy: 0.9957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Network Evaluation"
      ],
      "metadata": {
        "id": "RGYXD31_m7gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#can get history of training\n",
        "history.history.keys()\n",
        "plt.plot(history.history['loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "f12DrGDmnAIP",
        "outputId": "e292124f-b252-44f4-e40c-5024604b01ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8d08126980>]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "wbu66MIcn0b2",
        "outputId": "6268137a-370f-4010-b374-13ca4a57f1a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f8d081b04c0>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_test = network.evaluate(X_test, Y_test) #98 % accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtFrTaYTn2XV",
        "outputId": "098a9c5a-9ae0-4f8d-e131-de687568c59d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0850 - accuracy: 0.9792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = network.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCxoQEgXoF6n",
        "outputId": "d48e5fa2-bd7d-4617-b991-ebb55d0a643b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[0] #10 values, 1 value for each of the neurons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWY6OSl9oKam",
        "outputId": "25e06e83-8404-4688-9e12-3adf549eaef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.4377793e-13, 2.2570462e-10, 6.9518401e-12, 1.8399010e-09,\n",
              "       1.9897617e-15, 7.3961094e-14, 6.7543614e-19, 9.9999994e-01,\n",
              "       4.1375903e-13, 1.4369314e-09], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_test[0].reshape(28,28))\n",
        "print(\"Class: \" + str(Y_test[0])) #looking at the prediction outputs, largest probability is in position 7 from network, so this matches\n",
        "print(np.argmax(predictions[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "a6At2H4EoQA9",
        "outputId": "503323f9-820f-45b2-c8c8-4fb4f569d434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Network"
      ],
      "metadata": {
        "id": "n5lpvaFrLyzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Has support for convolutional networks and recurrent networks. Deep Learning is supported, and has GPU support; is an alternative to TensorFlow."
      ],
      "metadata": {
        "id": "KJuoGDyTii9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "5g5dbNJbL2Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__\n",
        "#cu indicates support for GPU's"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pfTs_7doi3kq",
        "outputId": "b53eb1ae-b1b7-46d0-8e72-11d3e595682f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.1+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import torch.nn as nn #means Neural Network and has the main functionalities"
      ],
      "metadata": {
        "id": "k60ZetbOjYh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "breast = datasets.load_breast_cancer()"
      ],
      "metadata": {
        "id": "lZKwScSSjrUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "breast.data\n",
        "breast.feature_names #all numeric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Me-bZuljvw4",
        "outputId": "770696fb-d6f4-49f7-e175-cb3c53b2c230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
              "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
              "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
              "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
              "       'smoothness error', 'compactness error', 'concavity error',\n",
              "       'concave points error', 'symmetry error',\n",
              "       'fractal dimension error', 'worst radius', 'worst texture',\n",
              "       'worst perimeter', 'worst area', 'worst smoothness',\n",
              "       'worst compactness', 'worst concavity', 'worst concave points',\n",
              "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "breast.target #Binary classification of malignant or not tumor\n",
        "breast.target_names\n",
        "\n",
        "inputs = breast.data\n",
        "inputs.shape #30 columns 569 rows\n",
        "\n",
        "outputs = breast.target\n",
        "outputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg6__M3ej6_A",
        "outputId": "d2df4041-614d-4ad1-c6cf-8d02cfd8f71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569,)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(inputs, outputs, test_size = 0.25) #25% for testing 75% learning, so 426 train and 143 test sets\n",
        "#we need to transform the data: pytorch has a unique variable to train the network"
      ],
      "metadata": {
        "id": "fsAF0aFKk7q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.tensor(x_train, dtype=torch.float)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzEyPPUPlYSI",
        "outputId": "e7c5758e-db3e-414c-c764-c6a72b0bf212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-0e87e75edc60>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_train = torch.tensor(x_train, dtype=torch.float)\n",
            "<ipython-input-71-0e87e75edc60>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_train = torch.tensor(y_train, dtype=torch.float)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(x_train) #good to go"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgMIK-fNljED",
        "outputId": "18e8bd55-e54b-4045-af9f-619b326b4210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#need to use another dataset for pytorch\n",
        "dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "type(dataset) #good format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugGHZUhyllqM",
        "outputId": "e52da713-e9df-4864-ee74-573b45c2bf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.TensorDataset"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size = 10) #train_loader goes through instances while training and is the k-fold gradient descent"
      ],
      "metadata": {
        "id": "wCpTb5jHl2do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30 neurons input -> 16 neurons -> 16 more neurons -> 1 neuron output layer\n",
        "network = nn.Sequential(nn.Linear(in_features = 30, out_features = 16), nn.Sigmoid(), nn.Linear(16,16), nn.Sigmoid(), nn.Linear(16, 1), nn.Sigmoid()) #can apply activation function\n",
        "#structure is now done"
      ],
      "metadata": {
        "id": "RsNTVutdv4RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network.parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgq1zQUFw4Eg",
        "outputId": "4b91c708-dc49-4b64-b26a-c6319d6c3e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Sequential(\n",
              "  (0): Linear(in_features=30, out_features=16, bias=True)\n",
              "  (1): Sigmoid()\n",
              "  (2): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (3): Sigmoid()\n",
              "  (4): Linear(in_features=16, out_features=1, bias=True)\n",
              "  (5): Sigmoid()\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now for loss function: binary cross entropy loss\n",
        "loss_function = nn.BCELoss()"
      ],
      "metadata": {
        "id": "0SiDdAMtw8t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(network.parameters(), lr = 0.001)\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0\n",
        "  for data in train_loader:\n",
        "    inputs, outputs = data\n",
        "    print(inputs)\n",
        "    print(\"----\")\n",
        "    print(outputs)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions = network.forward(inputs)\n",
        "    loss = loss_function(predictions, outputs)\n",
        "    loss.backwards()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "  print(\"Epoch: \" + str(epoch +1) + \"loss: \" + str(running_loss/len(train_loader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZiFz4TDCxVhP",
        "outputId": "cfe0df59-1687-4b78-d720-95fcca762768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.3170e+01, 1.8660e+01, 8.5980e+01, 5.3460e+02, 1.1580e-01, 1.2310e-01,\n",
            "         1.2260e-01, 7.3400e-02, 2.1280e-01, 6.7770e-02, 2.8710e-01, 8.9370e-01,\n",
            "         1.8970e+00, 2.4250e+01, 6.5320e-03, 2.3360e-02, 2.9050e-02, 1.2150e-02,\n",
            "         1.7430e-02, 3.6430e-03, 1.5670e+01, 2.7950e+01, 1.0280e+02, 7.5940e+02,\n",
            "         1.7860e-01, 4.1660e-01, 5.0060e-01, 2.0880e-01, 3.9000e-01, 1.1790e-01],\n",
            "        [2.1160e+01, 2.3040e+01, 1.3720e+02, 1.4040e+03, 9.4280e-02, 1.0220e-01,\n",
            "         1.0970e-01, 8.6320e-02, 1.7690e-01, 5.2780e-02, 6.9170e-01, 1.1270e+00,\n",
            "         4.3030e+00, 9.3990e+01, 4.7280e-03, 1.2590e-02, 1.7150e-02, 1.0380e-02,\n",
            "         1.0830e-02, 1.9870e-03, 2.9170e+01, 3.5590e+01, 1.8800e+02, 2.6150e+03,\n",
            "         1.4010e-01, 2.6000e-01, 3.1550e-01, 2.0090e-01, 2.8220e-01, 7.5260e-02],\n",
            "        [1.4190e+01, 2.3810e+01, 9.2870e+01, 6.1070e+02, 9.4630e-02, 1.3060e-01,\n",
            "         1.1150e-01, 6.4620e-02, 2.2350e-01, 6.4330e-02, 4.2070e-01, 1.8450e+00,\n",
            "         3.5340e+00, 3.1000e+01, 1.0880e-02, 3.7100e-02, 3.6880e-02, 1.6270e-02,\n",
            "         4.4990e-02, 4.7680e-03, 1.6860e+01, 3.4850e+01, 1.1500e+02, 8.1130e+02,\n",
            "         1.5590e-01, 4.0590e-01, 3.7440e-01, 1.7720e-01, 4.7240e-01, 1.0260e-01],\n",
            "        [1.5370e+01, 2.2760e+01, 1.0020e+02, 7.2820e+02, 9.2000e-02, 1.0360e-01,\n",
            "         1.1220e-01, 7.4830e-02, 1.7170e-01, 6.0970e-02, 3.1290e-01, 8.4130e-01,\n",
            "         2.0750e+00, 2.9440e+01, 9.8820e-03, 2.4440e-02, 4.5310e-02, 1.7630e-02,\n",
            "         2.4710e-02, 2.1420e-03, 1.6430e+01, 2.5840e+01, 1.0750e+02, 8.3090e+02,\n",
            "         1.2570e-01, 1.9970e-01, 2.8460e-01, 1.4760e-01, 2.5560e-01, 6.8280e-02],\n",
            "        [1.2620e+01, 2.3970e+01, 8.1350e+01, 4.9640e+02, 7.9030e-02, 7.5290e-02,\n",
            "         5.4380e-02, 2.0360e-02, 1.5140e-01, 6.0190e-02, 2.4490e-01, 1.0660e+00,\n",
            "         1.4450e+00, 1.8510e+01, 5.1690e-03, 2.2940e-02, 3.0160e-02, 8.6910e-03,\n",
            "         1.3650e-02, 3.4070e-03, 1.4200e+01, 3.1310e+01, 9.0670e+01, 6.2400e+02,\n",
            "         1.2270e-01, 3.4540e-01, 3.9110e-01, 1.1800e-01, 2.8260e-01, 9.5850e-02],\n",
            "        [9.7870e+00, 1.9940e+01, 6.2110e+01, 2.9450e+02, 1.0240e-01, 5.3010e-02,\n",
            "         6.8290e-03, 7.9370e-03, 1.3500e-01, 6.8900e-02, 3.3500e-01, 2.0430e+00,\n",
            "         2.1320e+00, 2.0050e+01, 1.1130e-02, 1.4630e-02, 5.3080e-03, 5.2500e-03,\n",
            "         1.8010e-02, 5.6670e-03, 1.0920e+01, 2.6290e+01, 6.8810e+01, 3.6610e+02,\n",
            "         1.3160e-01, 9.4730e-02, 2.0490e-02, 2.3810e-02, 1.9340e-01, 8.9880e-02],\n",
            "        [1.2760e+01, 1.3370e+01, 8.2290e+01, 5.0410e+02, 8.7940e-02, 7.9480e-02,\n",
            "         4.0520e-02, 2.5480e-02, 1.6010e-01, 6.1400e-02, 3.2650e-01, 6.5940e-01,\n",
            "         2.3460e+00, 2.5180e+01, 6.4940e-03, 2.7680e-02, 3.1370e-02, 1.0690e-02,\n",
            "         1.7310e-02, 4.3920e-03, 1.4190e+01, 1.6400e+01, 9.2040e+01, 6.1880e+02,\n",
            "         1.1940e-01, 2.2080e-01, 1.7690e-01, 8.4110e-02, 2.5640e-01, 8.2530e-02],\n",
            "        [1.1670e+01, 2.0020e+01, 7.5210e+01, 4.1620e+02, 1.0160e-01, 9.4530e-02,\n",
            "         4.2000e-02, 2.1570e-02, 1.8590e-01, 6.4610e-02, 2.0670e-01, 8.7450e-01,\n",
            "         1.3930e+00, 1.5340e+01, 5.2510e-03, 1.7270e-02, 1.8400e-02, 5.2980e-03,\n",
            "         1.4490e-02, 2.6710e-03, 1.3350e+01, 2.8810e+01, 8.7000e+01, 5.5060e+02,\n",
            "         1.5500e-01, 2.9640e-01, 2.7580e-01, 8.1200e-02, 3.2060e-01, 8.9500e-02],\n",
            "        [1.3620e+01, 2.3230e+01, 8.7190e+01, 5.7320e+02, 9.2460e-02, 6.7470e-02,\n",
            "         2.9740e-02, 2.4430e-02, 1.6640e-01, 5.8010e-02, 3.4600e-01, 1.3360e+00,\n",
            "         2.0660e+00, 3.1240e+01, 5.8680e-03, 2.0990e-02, 2.0210e-02, 9.0640e-03,\n",
            "         2.0870e-02, 2.5830e-03, 1.5350e+01, 2.9090e+01, 9.7580e+01, 7.2980e+02,\n",
            "         1.2160e-01, 1.5170e-01, 1.0490e-01, 7.1740e-02, 2.6420e-01, 6.9530e-02],\n",
            "        [1.0950e+01, 2.1350e+01, 7.1900e+01, 3.7110e+02, 1.2270e-01, 1.2180e-01,\n",
            "         1.0440e-01, 5.6690e-02, 1.8950e-01, 6.8700e-02, 2.3660e-01, 1.4280e+00,\n",
            "         1.8220e+00, 1.6970e+01, 8.0640e-03, 1.7640e-02, 2.5950e-02, 1.0370e-02,\n",
            "         1.3570e-02, 3.0400e-03, 1.2840e+01, 3.5340e+01, 8.7220e+01, 5.1400e+02,\n",
            "         1.9090e-01, 2.6980e-01, 4.0230e-01, 1.4240e-01, 2.9640e-01, 9.6060e-02]])\n",
            "----\n",
            "tensor([0., 0., 0., 0., 1., 1., 1., 1., 1., 0.])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-7bb5d54f5142>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3087\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3089\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   3090\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3091\u001b[0m             \u001b[0;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])) is deprecated. Please ensure they have the same size."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlEDdas4x2c-",
        "outputId": "ca327625-36e7-4a1f-c81f-d900ebfb1b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=30, out_features=16, bias=True)\n",
              "  (1): Sigmoid()\n",
              "  (2): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (3): Sigmoid()\n",
              "  (4): Linear(in_features=16, out_features=1, bias=True)\n",
              "  (5): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = torch.tensor(x_test, dtype = torch.float)"
      ],
      "metadata": {
        "id": "-hMFowgL-386"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = network.forward(x_test)\n",
        "predictions #close to 1 is cancer bad, closer to 0 is not as bad cancer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqiPcpPE_Bfv",
        "outputId": "24ac6d78-0806-4986-aa63-4aa048a6edd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5202],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5204],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5204],\n",
              "        [0.5204],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5204],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5204],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5157],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5204],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5206],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5206],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203],\n",
              "        [0.5203]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions  = np.array(predictions > 0.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWQVyLxU_QcY",
        "outputId": "b0a7bdb9-d8d6-498a-e1d5-035e57abc258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [ True]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRc8XC7a_XBc",
        "outputId": "a27353ca-1ebf-48a6-d0d5-fe9b5c9aa99a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
              "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
              "       1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
              "       1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
              "       1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
              "       0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y_test, predictions))\n",
        "\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "cm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye9jXMcN_Zk0",
        "outputId": "0461cc23-bd75-4b33-f042-36d5f92426c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6223776223776224\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0, 54],\n",
              "       [ 0, 89]])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    }
  ]
}